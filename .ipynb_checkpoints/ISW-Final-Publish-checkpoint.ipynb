{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "from scipy import integrate\n",
    "import healpy as hp\n",
    "import camb\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.special import spherical_jn\n",
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing the Millennium XXL - LCDM initial power ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h=0.6774\n",
    "\n",
    "#Set up a new set of parameters for CAMB\n",
    "pars = camb.CAMBparams()\n",
    "#This function sets up CosmoMC-like settings, with one massive neutrino and helium set using BBN consistency\n",
    "pars.set_cosmology(H0=67.74, ombh2=0.0223, omch2=0.1188, mnu=0.06, omk=0, tau=0.066)\n",
    "pars.InitPower.set_params(As=2e-09*(0.8159/0.78835443)**2, ns=0.9667, r=0.0)\n",
    "#RBeck - here no tensor modes are computed\n",
    "\n",
    "#Not non-linear corrections couples to smaller scales than you want - RBeck Ok what k to use then?\n",
    "pars.set_matter_power(redshifts=[1.0/0.104713-1.0], kmax=1500.0)\n",
    "pars.NonLinear = camb.model.NonLinear_none\n",
    "\n",
    "results = camb.get_results(pars)\n",
    "kVect, zVect, p_kVect = results.get_linear_matter_power_spectrum(hubble_units=False, have_power_spectra=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy.fft as fft\n",
    "\n",
    "def loadMilleniumDensity(filePath):\n",
    "\n",
    "    datafile=open(filePath, 'rb')\n",
    "\n",
    "    scaleFactor=np.fromfile(datafile, dtype=np.dtype('f4'), count=1, sep='')\n",
    "    cellNum=np.fromfile(datafile, dtype=np.dtype('i4'), count=1, sep='')\n",
    "\n",
    "    density=8.0*np.fromfile(datafile, dtype=np.dtype('f4'), count=cellNum[0]**3, sep='').reshape(([cellNum[0]]*3))\n",
    "\n",
    "    datafile.close()\n",
    "\n",
    "    return density\n",
    "\n",
    "pointMassCount=303464448000.0\n",
    "\n",
    "h_MXXL=0.73\n",
    "\n",
    "boxSize=3000/h_MXXL # scaling box size in Mpc to "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 0 into shape (1024,1024,1024)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-cfa07c9a4773>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgridDensities\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mloadMilleniumDensity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./data/MillenniumXXL/density_1024_014.dat'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# In count/cell volume\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#gridDensities=loadMilleniumDensity('data/MillenniumXXL/density_1024_063.dat') # In count/cell volume\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mlinearGridSize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgridDensities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-04fecf0c8471>\u001b[0m in \u001b[0;36mloadMilleniumDensity\u001b[0;34m(filePath)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mcellNum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatafile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'i4'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mdensity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8.0\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatafile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'f4'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcellNum\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcellNum\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mdatafile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 0 into shape (1024,1024,1024)"
     ]
    }
   ],
   "source": [
    "gridDensities=loadMilleniumDensity('data/MillenniumXXL/density_1024_014.dat') # In count/cell volume\n",
    "#gridDensities=loadMilleniumDensity('data/MillenniumXXL/density_1024_063.dat') # In count/cell volume\n",
    "\n",
    "linearGridSize=gridDensities.shape[0]\n",
    "\n",
    "averageDensity=pointMassCount/(linearGridSize)**3 # In count/cell volume\n",
    "\n",
    "delta_r_855=gridDensities/averageDensity-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "\n",
    "#Power spectrum computation routine\n",
    "def pk(d, boxsize=500.0,bin2fact=1.0/16.0, filename='',getnuminbin=False,overwrite=True,\n",
    "       checkmean=False,getxi=False):\n",
    "\n",
    "    #print filename\n",
    "\n",
    "    if (os.path.isfile(filename))*(overwrite == False):\n",
    "        #print 'not calculating'\n",
    "        p = np.loadtxt(filename)\n",
    "        kmean = p[:,0]\n",
    "        pk = p[:,1]\n",
    "        numinbin = p[:,2]\n",
    "\n",
    "    else:\n",
    "        #print 'calculating'\n",
    "\n",
    "        if (checkmean):\n",
    "            meanden = np.mean(d.flatten())\n",
    "            if (meanden != 0.):\n",
    "                print('Mean = '+str(meanden)+'.  Subtracting it off.')\n",
    "                d -= meanden\n",
    "        dk = np.fft.rfftn(d)\n",
    "        s = d.shape\n",
    "        sk = dk.shape\n",
    "        dk2 = (dk*np.conjugate(dk)).astype(np.float64)\n",
    "        #M.pcolor(np.abs(dk[:,:,0]))\n",
    "\n",
    "        dk2 = dk2.flatten()\n",
    "        \n",
    "\n",
    "        # need to double-count along the z-axis\n",
    "        \n",
    "        kmin = 2.*np.pi/boxsize\n",
    "\n",
    "        if (len(s) == 3):\n",
    "            a = np.fromfunction(lambda x,y,z:x, sk).astype(np.float64)\n",
    "            a[np.where(a > s[0]/2)] -= s[0]\n",
    "            b = np.fromfunction(lambda x,y,z:y, sk).astype(np.float64)\n",
    "            b[np.where(b > s[1]/2)] -= s[1]\n",
    "            c = np.fromfunction(lambda x,y,z:z, sk).astype(np.float64)\n",
    "            c[np.where(c > s[2]/2)] -= s[2]\n",
    "            # half-count cells on the z-axis\n",
    "\n",
    "            k = kmin*np.sqrt((a**2+b**2+c**2).flatten()).astype(np.float64)\n",
    "\n",
    "        elif (len(s) == 2):\n",
    "            b = np.fromfunction(lambda y,z:y, sk).astype(np.float64)\n",
    "            b[np.where(b > s[0]/2)] -= s[0]\n",
    "            c = np.fromfunction(lambda y,z:z, sk).astype(np.float64)\n",
    "            c[np.where(c > s[1]/2)] -= s[1]\n",
    "\n",
    "            k = kmin*np.sqrt((b**2+c**2).flatten()).astype(np.float64)\n",
    "        \n",
    "\n",
    "        index = np.argsort(k)\n",
    "\n",
    "        k = k[index]\n",
    "        dk2 = dk2[index]\n",
    "\n",
    "        c0 = 0.*c.flatten() + 1.\n",
    "        c0[np.where(c.flatten() == 0.)] -= 0.5\n",
    "        c0 = c0[index]\n",
    "\n",
    "        log2 = np.log(2.)\n",
    "    \n",
    "        binedges = kmin*2.**np.arange(-bin2fact/2.,np.log(k[-1]/kmin)/log2,bin2fact)\n",
    "        cuts = np.searchsorted(k,binedges)\n",
    "        numinbin = 0.*binedges\n",
    "        pk = 0.*binedges\n",
    "        kmean = 0.*binedges\n",
    "        nbins = len(binedges)\n",
    "\n",
    "        for i in np.arange(0,nbins-1):\n",
    "            if (cuts[i+1] > cuts[i]):\n",
    "                numinbin[i] = np.sum(c0[cuts[i]:cuts[i+1]])\n",
    "                pk[i] = np.sum(c0[cuts[i]:cuts[i+1]]*dk2[cuts[i]:cuts[i+1]])\n",
    "                kmean[i] = np.sum(c0[cuts[i]:cuts[i+1]]*k[cuts[i]:cuts[i+1]])\n",
    "\n",
    "        wn0 = np.where(numinbin > 0.)[0]\n",
    "        pk = pk[wn0]; kmean = kmean[wn0]; numinbin = numinbin[wn0]\n",
    "        pk /= numinbin\n",
    "        kmean /= numinbin\n",
    "\n",
    "        pk *= boxsize**3/np.prod(np.array(s).astype(np.float64))**2\n",
    "\n",
    "        if filename != '':\n",
    "            np.savetxt(filename, np.transpose([kmean,pk,numinbin]))\n",
    "\n",
    "    if (getnuminbin):\n",
    "        return kmean,pk,numinbin\n",
    "    else:\n",
    "        return kmean,pk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(kEmp_855,p_kEmp_855,numInBin)=pk(delta_r_855,boxsize=boxSize,getnuminbin=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_kLCDM=interp1d(kVect.astype(np.float64)*h, p_kVect[0,:], kind='cubic', fill_value='extrapolate')(kEmp_855)\n",
    "\n",
    "np.median(p_kLCDM/p_kEmp_855)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in cosmologies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "startScaleFactor=0.104713\n",
    "\n",
    "speedOfLightSI = 299792458.0 \n",
    "MpcInMeters = 3.08568025e22\n",
    "GyrInSeconds = 3.1536e16\n",
    "\n",
    "speedOfLightMpcGyr = speedOfLightSI/MpcInMeters*GyrInSeconds\n",
    "\n",
    "CMBTemp=2.726\n",
    "\n",
    "h=0.6774"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def growthFunc(zz,HH):\n",
    "    # this is the factor to convert the scale-less analytical growth factor to real\n",
    "    H_zMax_scaleLess = (2.0/3.0*(1.0+zz[-1])**(3.0/2.0))\n",
    "    ff = 1.0/(HH[-1] / H_zMax_scaleLess )**3 #This is essentially t_0^3 in the EdS formula, fitted at the highest z\n",
    "\n",
    "    arg_EdS = lambda x: 1.0/(1.0+x)**(7.0/2.0)\n",
    "    D_zMax_inf_EdS_raw, err = integrate.quad(arg_EdS, zz[-1], np.inf)\n",
    "    D_zMax_inf_EdS_raw = 27.0/8.0 * D_zMax_inf_EdS_raw\n",
    "\n",
    "    #print(err)\n",
    "    \n",
    "    D_zMax_inf_EdS = ff*D_zMax_inf_EdS_raw\n",
    "\n",
    "    D_0_z = integrate.cumtrapz(np.divide(1.0+zz,HH**3),zz,initial=0)\n",
    "    D_0_9 = integrate.trapz(np.divide(1.0+zz,HH**3),zz)\n",
    "\n",
    "    D1 = HH/HH[0] * \\\n",
    "        (D_0_9 + D_zMax_inf_EdS - D_0_z) \\\n",
    "        / (D_0_9 + D_zMax_inf_EdS)\n",
    "        \n",
    "    return D1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CosmoContainer:\n",
    "    \n",
    "    \n",
    "    def __init__(self, scaleFactorVect, tauVect, D1, onePlusZD1_dtauVect, kVect, p_kVect, H0, omega_m):\n",
    "        \n",
    "        self.zCurve_ = 1.0/scaleFactorVect-1.0\n",
    "        \n",
    "        self.taus_ = tauVect\n",
    "        \n",
    "        self.D1_ = D1\n",
    "        self.onePlusZD1_dtauVect_ = onePlusZD1_dtauVect\n",
    "        \n",
    "        self.H0_SI_ = H0*1000/MpcInMeters\n",
    "        self.omega_m_ = omega_m\n",
    "        \n",
    "        self.kLimits_ = [np.amin(kVect.astype(np.float64)*h),np.amax(kVect.astype(np.float64)*h)]\n",
    "\n",
    "        self.p_kFunc_ = interp1d(kVect.astype(np.float64)*h, \n",
    "                                 p_kVect[0,:]*MpcInMeters**3, \n",
    "                                 kind='cubic', \n",
    "                                 fill_value='extrapolate')\n",
    "\n",
    "    def createFromCambResults(scaleFactorVect, cambResults, smoothingSigma, isLinear=True):\n",
    "        #isLinear==False may require tuning of parameters\n",
    "        \n",
    "        zCurve=1.0/scaleFactorVect-1.0\n",
    "        \n",
    "        tauVect = cambResults.conformal_time(zCurve)/speedOfLightMpcGyr\n",
    "        \n",
    "        hubbleFactors = cambResults.hubble_parameter(zCurve)\n",
    "\n",
    "        D1_sim=np.flipud(growthFunc(np.flipud(zCurve),np.flipud(hubbleFactors)))\n",
    "\n",
    "        onePlusZD1_dtau_sim=np.gradient((1.0/scaleFactorVect)*D1_sim,edge_order=2)/np.gradient(tauVect,edge_order=2)\n",
    "\n",
    "        onePlusZD1_dtau_sim_smooth=gaussian_filter1d(onePlusZD1_dtau_sim,sigma=smoothingSigma, mode='nearest')\n",
    "    \n",
    "        if isLinear:\n",
    "            \n",
    "            kVect, zVect, p_kVect = cambResults.get_linear_matter_power_spectrum(hubble_units=False, have_power_spectra=True)\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            kVect, zVect, p_kVect = cambResults.get_matter_power_spectrum(have_power_spectra=True, \n",
    "                                                                          minkh=2e-5, \n",
    "                                                                          maxkh=1500, \n",
    "                                                                          npoints = 500)\n",
    "            \n",
    "            p_kVect/=(cambResults.get_params().H0/100)**3\n",
    "    \n",
    "        return CosmoContainer(scaleFactorVect, tauVect, D1_sim, onePlusZD1_dtau_sim_smooth, kVect, p_kVect,\n",
    "                              cambResults.get_params().H0, \n",
    "                              cambResults.get_params().omegab+cambResults.get_params().omegac)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Higher resolution LCDM cosmology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sims=['LCDM']\n",
    "\n",
    "times=[]\n",
    "scaleFactors=[]\n",
    "hubbleFactors=[]\n",
    "omega_m_effs=[]\n",
    "\n",
    "snapshotNames=[]\n",
    "\n",
    "for sim in sims:\n",
    "    globalSimParams=np.genfromtxt('data/timeCurve_'+sim+'.txt',dtype='str')\n",
    "    \n",
    "    times.append(np.array([float(f) for f in globalSimParams[:,0]]))\n",
    "    scaleFactors.append(np.array([float(f) for f in globalSimParams[:,1]]))\n",
    "    hubbleFactors.append(np.array([float(f) for f in globalSimParams[:,3]]))\n",
    "    omega_m_effs.append(np.array([float(f) for f in globalSimParams[:,4]]))\n",
    "\n",
    "    snapshotNames.append(globalSimParams[:,2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "#aStepSize=0.001\n",
    "aStepSize=0.00005\n",
    "\n",
    "aVect=np.flipud(np.arange(1.0,startScaleFactor-aStepSize,-aStepSize))\n",
    "\n",
    "smoothingSigma=0.01/aStepSize\n",
    "\n",
    "for i in range(len(sims)):\n",
    "    times[i]=interp1d(scaleFactors[i], times[i], kind='cubic', fill_value='extrapolate')(aVect)\n",
    "    hubbleFactors[i]=interp1d(scaleFactors[i], hubbleFactors[i], kind='cubic', fill_value='extrapolate')(aVect)\n",
    "    omega_m_effs[i]=interp1d(scaleFactors[i], omega_m_effs[i], kind='cubic', fill_value='extrapolate')(aVect)\n",
    "\n",
    "    scaleFactors[i]=copy.deepcopy(aVect)\n",
    "\n",
    "    \n",
    "taus=[]\n",
    "    \n",
    "for i in range(len(sims)):\n",
    "    \n",
    "    taus.append(3*times[i][0]/scaleFactors[i][0]+integrate.cumtrapz(1.0/scaleFactors[i],times[i],initial=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "D1_sim=[]\n",
    "onePlusZD1_dz_sim=[]\n",
    "onePlusZD1_dz_sim_smooth=[]\n",
    "\n",
    "onePlusZD1_dtau_sim=[]\n",
    "onePlusZD1_dtau_sim_smooth=[]\n",
    "\n",
    "beta_sim=[]\n",
    "beta_sim_smooth=[]\n",
    "beta_sim_interp=[]\n",
    "\n",
    "for i in range(len(sims)):\n",
    "    \n",
    "    D1_sim.append(np.flipud(growthFunc(np.flipud(1.0/scaleFactors[i]-1.0),np.flipud(hubbleFactors[i]))))\n",
    "    \n",
    "    onePlusZD1_dz_sim.append(np.gradient((1.0/scaleFactors[i])*D1_sim[i],edge_order=2)/\n",
    "                             np.gradient(1.0/scaleFactors[i]-1.0,edge_order=2))\n",
    "    \n",
    "    onePlusZD1_dz_sim_smooth.append(gaussian_filter1d(onePlusZD1_dz_sim[i],sigma=smoothingSigma, mode='nearest'))\n",
    "  \n",
    "    onePlusZD1_dtau_sim.append(np.gradient((1.0/scaleFactors[i])*D1_sim[i],edge_order=2)/\n",
    "                               np.gradient(taus[i],edge_order=2))\n",
    "    \n",
    "    onePlusZD1_dtau_sim_smooth.append(gaussian_filter1d(onePlusZD1_dtau_sim[i],sigma=smoothingSigma, mode='nearest'))\n",
    "\n",
    "    beta_sim.append(np.gradient(np.log(D1_sim[i]),edge_order=2)/np.gradient(np.log(scaleFactors[i]),edge_order=2))\n",
    "    \n",
    "    beta_sim_smooth.append(gaussian_filter1d(beta_sim[i],sigma=smoothingSigma, mode='nearest'))\n",
    "    \n",
    "    beta_sim_interp.append(interp1d(taus[i], beta_sim_smooth[i], kind='cubic', fill_value='extrapolate'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(sims)):\n",
    "\n",
    "    times[i][0]=interp1d(scaleFactors[i], \n",
    "                         times[i], \n",
    "                         kind='cubic', fill_value='extrapolate')(startScaleFactor)\n",
    "    \n",
    "    hubbleFactors[i][0]=interp1d(scaleFactors[i], \n",
    "                                 hubbleFactors[i], \n",
    "                                 kind='cubic', fill_value='extrapolate')(startScaleFactor)\n",
    "    \n",
    "    omega_m_effs[i][0]=interp1d(scaleFactors[i], \n",
    "                                omega_m_effs[i], \n",
    "                                kind='cubic', fill_value='extrapolate')(startScaleFactor)\n",
    "\n",
    "\n",
    "    taus[i][0]=interp1d(scaleFactors[i], \n",
    "                        taus[i], \n",
    "                        kind='cubic', fill_value='extrapolate')(startScaleFactor)\n",
    "\n",
    "    \n",
    "    D1_sim[i][0]=interp1d(scaleFactors[i], \n",
    "                          D1_sim[i], \n",
    "                          kind='cubic', fill_value='extrapolate')(startScaleFactor)\n",
    "    \n",
    "    onePlusZD1_dz_sim_smooth[i][0]=interp1d(scaleFactors[i], \n",
    "                                            onePlusZD1_dz_sim_smooth[i], \n",
    "                                            kind='cubic', fill_value='extrapolate')(startScaleFactor)\n",
    "    \n",
    "    onePlusZD1_dtau_sim_smooth[i][0]=interp1d(scaleFactors[i], \n",
    "                                              onePlusZD1_dtau_sim_smooth[i], \n",
    "                                              kind='cubic', fill_value='extrapolate')(startScaleFactor)\n",
    "    \n",
    "    beta_sim_smooth[i][0]=interp1d(scaleFactors[i], \n",
    "                                   beta_sim_smooth[i], \n",
    "                                   kind='cubic', fill_value='extrapolate')(startScaleFactor)\n",
    "    \n",
    "    scaleFactors[i][0]=startScaleFactor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Set up a new set of parameters for CAMB\n",
    "pars = camb.CAMBparams()\n",
    "#This function sets up CosmoMC-like settings, with one massive neutrino and helium set using BBN consistency\n",
    "pars.set_cosmology(H0=67.74, ombh2=0.0223, omch2=0.1188, mnu=0.06, omk=0, tau=0.066)\n",
    "pars.InitPower.set_params(As=2e-09*(0.8159/0.78835443)**2, ns=0.9667, r=0.0)\n",
    "#RBeck - here no tensor modes are computed\n",
    "\n",
    "#Not non-linear corrections couples to smaller scales than you want - RBeck Ok what k to use then?\n",
    "pars.set_matter_power(redshifts=[1.0/startScaleFactor-1.0], kmax=1500.0)\n",
    "pars.NonLinear = camb.model.NonLinear_none\n",
    "\n",
    "results = camb.get_results(pars)\n",
    "kVect, zVect, p_kVect = results.get_linear_matter_power_spectrum(hubble_units=False, have_power_spectra=True)\n",
    "\n",
    "p_kVect_sim=[]\n",
    "\n",
    "for i in range(len(sims)):\n",
    "\n",
    "    p_kVect_sim.append(p_kVect*(D1_sim[i][-1]/D1_sim[i][0])**2.0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cosmoContainerDict={}\n",
    "\n",
    "for i in range(len(sims)):\n",
    "\n",
    "    cosmoContainerDict[sims[i]+str(aStepSize)]=CosmoContainer(scaleFactors[i], taus[i], \n",
    "                                                              D1_sim[i], onePlusZD1_dtau_sim_smooth[i],\n",
    "                                                              kVect, p_kVect_sim[i], \n",
    "                                                              hubbleFactors[i][-1], omega_m_effs[i][-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular resolution cosmologies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sims=['BR','LCDM','EdS']\n",
    "simColors=['blue','red','green']\n",
    "simLabels=['AvERA','LCDM','EdS']\n",
    "\n",
    "times=[]\n",
    "scaleFactors=[]\n",
    "hubbleFactors=[]\n",
    "omega_m_effs=[]\n",
    "\n",
    "snapshotNames=[]\n",
    "\n",
    "for sim in sims:\n",
    "    globalSimParams=np.genfromtxt('data/timeCurve_'+sim+'.txt',dtype='str')\n",
    "    \n",
    "    times.append(np.array([float(f) for f in globalSimParams[:,0]]))\n",
    "    scaleFactors.append(np.array([float(f) for f in globalSimParams[:,1]]))\n",
    "    hubbleFactors.append(np.array([float(f) for f in globalSimParams[:,3]]))\n",
    "    omega_m_effs.append(np.array([float(f) for f in globalSimParams[:,4]]))\n",
    "\n",
    "    snapshotNames.append(globalSimParams[:,2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tags=['320k','625k','1080k']\n",
    "\n",
    "sims+=['Av320k','Av625k','Av1080k']\n",
    "simColors+=['Aqua','Turquoise','Teal']\n",
    "simLabels+=['Av320k','Av625k','Av1080k']\n",
    "\n",
    "for tag in tags:\n",
    "    globalSimParams=np.genfromtxt('data/timeCurve_'+tag+'_BR.txt',dtype='str')\n",
    "    \n",
    "    times.append(np.array([float(f) for f in globalSimParams[:,0]]))\n",
    "    scaleFactors.append(np.array([float(f) for f in globalSimParams[:,1]]))\n",
    "    hubbleFactors.append(np.array([float(f) for f in globalSimParams[:,3]]))\n",
    "    omega_m_effs.append(np.array([float(f) for f in globalSimParams[:,4]]))\n",
    "\n",
    "    snapshotNames.append(globalSimParams[:,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "aStepSize=0.001\n",
    "#aStepSize=0.00005\n",
    "\n",
    "aVect=np.flipud(np.arange(1.0,startScaleFactor-aStepSize,-aStepSize))\n",
    "\n",
    "smoothingSigma=0.01/aStepSize\n",
    "\n",
    "for i in range(len(sims)):\n",
    "    times[i]=interp1d(scaleFactors[i], times[i], kind='cubic', fill_value='extrapolate')(aVect)\n",
    "    hubbleFactors[i]=interp1d(scaleFactors[i], hubbleFactors[i], kind='cubic', fill_value='extrapolate')(aVect)\n",
    "    omega_m_effs[i]=interp1d(scaleFactors[i], omega_m_effs[i], kind='cubic', fill_value='extrapolate')(aVect)\n",
    "\n",
    "    scaleFactors[i]=copy.deepcopy(aVect)\n",
    "\n",
    "    \n",
    "taus=[]\n",
    "    \n",
    "for i in range(len(sims)):\n",
    "    \n",
    "    taus.append(3*times[i][0]/scaleFactors[i][0]+integrate.cumtrapz(1.0/scaleFactors[i],times[i],initial=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "D1_sim=[]\n",
    "onePlusZD1_dz_sim=[]\n",
    "onePlusZD1_dz_sim_smooth=[]\n",
    "\n",
    "onePlusZD1_dtau_sim=[]\n",
    "onePlusZD1_dtau_sim_smooth=[]\n",
    "\n",
    "beta_sim=[]\n",
    "beta_sim_smooth=[]\n",
    "beta_sim_interp=[]\n",
    "\n",
    "for i in range(len(sims)):\n",
    "    \n",
    "    D1_sim.append(np.flipud(growthFunc(np.flipud(1.0/scaleFactors[i]-1.0),np.flipud(hubbleFactors[i]))))\n",
    "    \n",
    "    onePlusZD1_dz_sim.append(np.gradient((1.0/scaleFactors[i])*D1_sim[i],edge_order=2)/\n",
    "                             np.gradient(1.0/scaleFactors[i]-1.0,edge_order=2))\n",
    "    \n",
    "    onePlusZD1_dz_sim_smooth.append(gaussian_filter1d(onePlusZD1_dz_sim[i],sigma=smoothingSigma, mode='nearest'))\n",
    "  \n",
    "    onePlusZD1_dtau_sim.append(np.gradient((1.0/scaleFactors[i])*D1_sim[i],edge_order=2)/\n",
    "                               np.gradient(taus[i],edge_order=2))\n",
    "    \n",
    "    onePlusZD1_dtau_sim_smooth.append(gaussian_filter1d(onePlusZD1_dtau_sim[i],sigma=smoothingSigma, mode='nearest'))\n",
    "\n",
    "    beta_sim.append(np.gradient(np.log(D1_sim[i]),edge_order=2)/np.gradient(np.log(scaleFactors[i]),edge_order=2))\n",
    "    \n",
    "    beta_sim_smooth.append(gaussian_filter1d(beta_sim[i],sigma=smoothingSigma, mode='nearest'))\n",
    "    \n",
    "    beta_sim_interp.append(interp1d(taus[i], beta_sim_smooth[i], kind='cubic', fill_value='extrapolate'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(sims)):\n",
    "\n",
    "    times[i][0]=interp1d(scaleFactors[i], \n",
    "                         times[i], \n",
    "                         kind='cubic', fill_value='extrapolate')(startScaleFactor)\n",
    "    \n",
    "    hubbleFactors[i][0]=interp1d(scaleFactors[i], \n",
    "                                 hubbleFactors[i], \n",
    "                                 kind='cubic', fill_value='extrapolate')(startScaleFactor)\n",
    "    \n",
    "    omega_m_effs[i][0]=interp1d(scaleFactors[i], \n",
    "                                omega_m_effs[i], \n",
    "                                kind='cubic', fill_value='extrapolate')(startScaleFactor)\n",
    "\n",
    "\n",
    "    taus[i][0]=interp1d(scaleFactors[i], \n",
    "                        taus[i], \n",
    "                        kind='cubic', fill_value='extrapolate')(startScaleFactor)\n",
    "\n",
    "    \n",
    "    D1_sim[i][0]=interp1d(scaleFactors[i], \n",
    "                          D1_sim[i], \n",
    "                          kind='cubic', fill_value='extrapolate')(startScaleFactor)\n",
    "    \n",
    "    onePlusZD1_dz_sim_smooth[i][0]=interp1d(scaleFactors[i], \n",
    "                                            onePlusZD1_dz_sim_smooth[i], \n",
    "                                            kind='cubic', fill_value='extrapolate')(startScaleFactor)\n",
    "    \n",
    "    onePlusZD1_dtau_sim_smooth[i][0]=interp1d(scaleFactors[i], \n",
    "                                              onePlusZD1_dtau_sim_smooth[i], \n",
    "                                              kind='cubic', fill_value='extrapolate')(startScaleFactor)\n",
    "    \n",
    "    beta_sim_smooth[i][0]=interp1d(scaleFactors[i], \n",
    "                                   beta_sim_smooth[i], \n",
    "                                   kind='cubic', fill_value='extrapolate')(startScaleFactor)\n",
    "    \n",
    "    scaleFactors[i][0]=startScaleFactor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Set up a new set of parameters for CAMB\n",
    "pars = camb.CAMBparams()\n",
    "#This function sets up CosmoMC-like settings, with one massive neutrino and helium set using BBN consistency\n",
    "pars.set_cosmology(H0=67.74, ombh2=0.0223, omch2=0.1188, mnu=0.06, omk=0, tau=0.066)\n",
    "pars.InitPower.set_params(As=2e-09*(0.8159/0.78835443)**2, ns=0.9667, r=0.0)\n",
    "#RBeck - here no tensor modes are computed\n",
    "\n",
    "#Not non-linear corrections couples to smaller scales than you want - RBeck Ok what k to use then?\n",
    "pars.set_matter_power(redshifts=[1.0/startScaleFactor-1.0], kmax=1500.0)\n",
    "pars.NonLinear = camb.model.NonLinear_none\n",
    "\n",
    "results = camb.get_results(pars)\n",
    "kVect, zVect, p_kVect = results.get_linear_matter_power_spectrum(hubble_units=False, have_power_spectra=True)\n",
    "\n",
    "p_kVect_sim=[]\n",
    "\n",
    "for i in range(len(sims)):\n",
    "\n",
    "    p_kVect_sim.append(p_kVect*(D1_sim[i][-1]/D1_sim[i][0])**2.0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#cosmoContainerDict={}\n",
    "\n",
    "for i in range(len(sims)):\n",
    "    \n",
    "    #sims[i]+str(aStepSize)\n",
    "    cosmoContainerDict[sims[i]]=CosmoContainer(scaleFactors[i], taus[i], \n",
    "                                               D1_sim[i], onePlusZD1_dtau_sim_smooth[i],\n",
    "                                               kVect, p_kVect_sim[i], \n",
    "                                               hubbleFactors[i][-1], omega_m_effs[i][-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(22,6))\n",
    "\n",
    "sims=['BR','LCDM']\n",
    "simColors=['blue','red']\n",
    "plotLabels=['AvERA','$\\Lambda$CDM']\n",
    "\n",
    "axs=[]\n",
    "\n",
    "axs.append(fig.add_subplot(131))\n",
    "axs.append(fig.add_subplot(132))\n",
    "axs.append(fig.add_subplot(133))\n",
    "\n",
    "for i in range(len(axs)):  \n",
    "    axs[i].set_xlim([16,47.2])\n",
    "    axs[i].set_xlabel('$\\\\tau \\, [\\mathrm{Gyr}]$', fontsize=24)\n",
    "    axs[i].tick_params(axis='both', labelsize=20)\n",
    "    axs[i].grid(True)\n",
    "    \n",
    "axs[0].set_ylabel('$\\\\beta(\\\\tau)$', fontsize=24)\n",
    "\n",
    "for i in range(len(sims)):\n",
    "    axs[0].plot(taus[i], beta_sim[i], 'k.', markersize=2)\n",
    "    axs[0].plot(taus[i], beta_sim_smooth[i], '-', color=simColors[i])\n",
    "\n",
    "axs[1].set_ylabel('$ \\\\frac{d}{d\\\\tau} \\\\left( \\\\frac{D}{a} \\\\right)$', fontsize=24)\n",
    "    \n",
    "for i in range(len(sims)):\n",
    "    axs[1].plot(taus[i], onePlusZD1_dtau_sim[i], 'k.', markersize=2)\n",
    "    axs[1].plot(taus[i], onePlusZD1_dtau_sim_smooth[i], '-', color=simColors[i])\n",
    "\n",
    "    \n",
    "axs[2].set_ylabel('$H(\\\\tau) \\, [\\mathrm{km \\, s^{-1} Mpc^{-1}}]$', fontsize=24)\n",
    "    \n",
    "for i in range(len(sims)):\n",
    "    axs[2].plot(taus[i], hubbleFactors[i], '-', color=simColors[i], label=plotLabels[i])\n",
    "\n",
    "axs[2].legend(shadow=False, fancybox=True, fontsize=24, loc=1)\n",
    "\n",
    "\n",
    "tau_a_func=interp1d(scaleFactors[1], taus[1],  kind='cubic', fill_value='extrapolate')\n",
    "\n",
    "for ax in axs:\n",
    "    \n",
    "    zVals=np.array([9,4,2,1,0])\n",
    "    \n",
    "    tauVals=tau_a_func(1.0/(1+zVals))\n",
    "    \n",
    "    #\"{0:d}\".format(zVals)\n",
    "    \n",
    "    axTop = ax.twiny()\n",
    "    axTop.set_xlim(ax.get_xlim())\n",
    "    \n",
    "    axTop.set_xticks(tauVals)\n",
    "    axTop.set_xticklabels(zVals)\n",
    "    axTop.tick_params(axis='x', labelsize=20)\n",
    "    axTop.set_xlabel('$z$', fontsize=24)\n",
    "    \n",
    "#axs[1].set_xlim([1,100])\n",
    "#axs[1].set_ylim([0,8])\n",
    "fig.tight_layout()\n",
    "\n",
    "fig.savefig('cosmology.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Afshordi 2004\n",
    "\n",
    "h=0.71\n",
    "\n",
    "omega_m=0.135/h**2\n",
    "\n",
    "omega_b=0.0224/h**2\n",
    "\n",
    "sigma_8=0.84\n",
    "\n",
    "#Set up a new set of parameters for CAMB\n",
    "pars = camb.CAMBparams()\n",
    "#This function sets up CosmoMC-like settings, with one massive neutrino and helium set using BBN consistency\n",
    "pars.set_cosmology(H0=h*100, ombh2=omega_b*h**2, omch2=(omega_m-omega_b)*h**2, omk=0, tau=0.17) #,mnu=0.06, tau=0.066)\n",
    "pars.InitPower.set_params(As=2e-09*(sigma_8/0.75736669)**2, ns=0.93, r=0.0)\n",
    "\n",
    "\n",
    "pars.set_matter_power(redshifts=[0.0], kmax=1500.0)\n",
    "\n",
    "#pars.NonLinear = camb.model.NonLinear_pk\n",
    "pars.NonLinear = camb.model.NonLinear_none\n",
    "\n",
    "results = camb.get_results(pars)\n",
    "\n",
    "cosmoContainerDict['Afshordi2004']=CosmoContainer.createFromCambResults(scaleFactors[1], results, smoothingSigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Francis 2010\n",
    "\n",
    "h=0.7\n",
    "\n",
    "omega_m=0.3\n",
    "\n",
    "omega_b=0.05\n",
    "\n",
    "sigma_8=0.75\n",
    "\n",
    "#Set up a new set of parameters for CAMB\n",
    "pars = camb.CAMBparams()\n",
    "#This function sets up CosmoMC-like settings, with one massive neutrino and helium set using BBN consistency\n",
    "pars.set_cosmology(H0=h*100, ombh2=omega_b*h**2, omch2=(omega_m-omega_b)*h**2, omk=0) #,mnu=0.06, tau=0.066)\n",
    "pars.InitPower.set_params(As=2e-09*(sigma_8/0.80682145)**2, ns=1.0, r=0.0)\n",
    "\n",
    "pars.set_matter_power(redshifts=[0.0], kmax=1500.0)\n",
    "\n",
    "#pars.NonLinear = camb.model.NonLinear_pk\n",
    "pars.NonLinear = camb.model.NonLinear_none\n",
    "\n",
    "\n",
    "results = camb.get_results(pars)\n",
    "\n",
    "cosmoContainerDict['Francis2010']=CosmoContainer.createFromCambResults(scaleFactors[1], results, smoothingSigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cosmological power spectrum integrals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getIndicesInRedshiftRange(zLimits, cosmoCont):\n",
    "    \n",
    "    boundaryIndices=np.argmin(abs(cosmoCont.zCurve_.reshape(-1,1)-zLimits),axis=0)\n",
    "    boundaryIndices.sort()\n",
    "\n",
    "    return np.arange(boundaryIndices[0],boundaryIndices[1]+1)\n",
    "\n",
    "def BesselCurve_tau(l,k,cosmoCont):\n",
    "\n",
    "    tauVect=cosmoCont.taus_\n",
    "    \n",
    "    return spherical_jn(l,(tauVect[-1]-tauVect)*speedOfLightMpcGyr*k)\n",
    "\n",
    "\n",
    "def G_ISW_tau(k,cosmoCont,bessel_tau):\n",
    "    \n",
    "    innerIntegrand=cosmoCont.onePlusZD1_dtauVect_*bessel_tau\n",
    "        \n",
    "    #The derivative by tau takes the dimension of tauVect\n",
    "    return -(3*(cosmoCont.H0_SI_**2)/(speedOfLightSI**2)*cosmoCont.omega_m_)*CMBTemp/(k**2)*(MpcInMeters**2)*innerIntegrand\n",
    "\n",
    "\n",
    "def G_Gal_tau(Pi,b,cosmoCont,bessel_tau):\n",
    "\n",
    "    innerIntegrand=b*Pi*cosmoCont.D1_*bessel_tau\n",
    "    \n",
    "    #The derivative by tau within Pi takes the dimension of tauVect\n",
    "    return innerIntegrand\n",
    "\n",
    "def C_lIntegrand(k,cosmoCont,G1,G2):\n",
    "    \n",
    "    return (2.0/math.pi)*cosmoCont.p_kFunc_(k)*(k**2)/(MpcInMeters**2)*G1*G2\n",
    "\n",
    "\n",
    "def C_l_Bessel(lVect, zLimits, kLimits, kRes, cosmoCont, corrType='TT', Pi=None, b=None, returnAsTauFunction=False):\n",
    "\n",
    "    kVect=np.logspace(np.log10(kLimits[0]),np.log10(kLimits[1]),kRes,base=10)\n",
    "\n",
    "    withinRange=getIndicesInRedshiftRange(zLimits, cosmoCont)\n",
    "    \n",
    "    C_l=[]\n",
    "    \n",
    "    for l in lVect:\n",
    "\n",
    "        outerIntegrand=[]\n",
    "\n",
    "        for k in kVect:\n",
    "\n",
    "            bessel_tau=BesselCurve_tau(l,k,cosmoCont)\n",
    "\n",
    "            if corrType=='TT':\n",
    "\n",
    "                G2_tau=G_ISW_tau(k,cosmoCont,bessel_tau)\n",
    "\n",
    "                G1_tau=G2_tau[withinRange]\n",
    "\n",
    "            elif corrType=='GG':\n",
    "\n",
    "                G2_tau=G_Gal_tau(Pi,b,cosmoCont,bessel_tau)\n",
    "\n",
    "                G1_tau=G2_tau[withinRange]\n",
    "\n",
    "            elif corrType=='GT':\n",
    "\n",
    "                G2_tau=G_ISW_tau(k,cosmoCont,bessel_tau)\n",
    "\n",
    "                G1_tau=G_Gal_tau(Pi,b,cosmoCont,bessel_tau)[withinRange]       \n",
    "\n",
    "            else:\n",
    "\n",
    "                raise ValueError('Invalid corrType value provided. Valid options: TT, GG and GT')\n",
    "\n",
    "            if returnAsTauFunction:\n",
    "                G1=G1_tau           \n",
    "            else:\n",
    "                G1=integrate.trapz(G1_tau,cosmoCont.taus_[withinRange])\n",
    "\n",
    "            G2=integrate.trapz(G2_tau,cosmoCont.taus_)\n",
    "\n",
    "            outerIntegrand.append(C_lIntegrand(k,cosmoCont,G1,G2))\n",
    "\n",
    "        C_l.append(integrate.trapz(np.array(outerIntegrand),kVect/MpcInMeters,axis=0))\n",
    "        \n",
    "    return np.array(C_l)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def G_ISW_Limber_tau(l,cosmoCont):\n",
    "\n",
    "    tauVect=cosmoCont.taus_\n",
    "    \n",
    "    tau_0=tauVect[-1]\n",
    "    \n",
    "    kInv=((tau_0-tauVect)*speedOfLightMpcGyr)/(l+0.5)\n",
    "    \n",
    "    #The derivative by tau takes the dimension of tauVect\n",
    "    return -((3*(cosmoCont.H0_SI_**2)/(speedOfLightSI**2)*cosmoCont.omega_m_)*CMBTemp*(kInv**2)*(MpcInMeters**2)*\n",
    "             cosmoCont.onePlusZD1_dtauVect_/speedOfLightSI)\n",
    "\n",
    "\n",
    "def G_Gal_Limber_tau(Pi,b,cosmoCont):\n",
    "\n",
    "    return b*Pi*cosmoCont.D1_/speedOfLightSI\n",
    "\n",
    "def C_lIntegrand_Limber(l,cosmoCont,G1,G2):\n",
    "    \n",
    "    tauVect=cosmoCont.taus_\n",
    "    \n",
    "    tau_0=tauVect[-1]\n",
    "    \n",
    "    chi=(tau_0-tauVect)*speedOfLightMpcGyr\n",
    "    chi[-1]=1.0\n",
    "    #This is to prevent division by zero. Knocked out by P_k[-1]=0.0 below\n",
    "    \n",
    "    P_k=np.empty(len(tauVect))\n",
    "    \n",
    "    P_k[:-1]=cosmoCont.p_kFunc_( (l+0.5)/((tau_0-tauVect[:-1])*speedOfLightMpcGyr) )\n",
    "    P_k[-1]=0.0\n",
    "    \n",
    "    return G1*G2/(chi**2)/(MpcInMeters**2)*P_k*speedOfLightSI/GyrInSeconds\n",
    "\n",
    "\n",
    "def C_l_Limber(lVect, zLimits, cosmoCont, corrType='TT', Pi=None, b=None, returnAsTauFunction=False):\n",
    "\n",
    "    withinRange=getIndicesInRedshiftRange(zLimits, cosmoCont)\n",
    "    \n",
    "    C_l=[]\n",
    "    \n",
    "    for l in lVect:\n",
    "\n",
    "        if corrType=='TT':\n",
    "\n",
    "            G2_tau=G_ISW_Limber_tau(l,cosmoCont)\n",
    "\n",
    "            G1_tau=G2_tau\n",
    "\n",
    "        elif corrType=='GG':\n",
    "\n",
    "            G2_tau=G_Gal_Limber_tau(Pi,b,cosmoCont)\n",
    "\n",
    "            G1_tau=G2_tau\n",
    "\n",
    "        elif corrType=='GT':\n",
    "\n",
    "            G2_tau=G_ISW_Limber_tau(l,cosmoCont)\n",
    "\n",
    "            G1_tau=G_Gal_Limber_tau(Pi,b,cosmoCont)\n",
    "\n",
    "        else:\n",
    "\n",
    "            raise ValueError('Invalid corrType value provided. Valid options: TT, GG and GT')\n",
    "\n",
    "        \n",
    "        C_l_tau=C_lIntegrand_Limber(l,cosmoCont,G1_tau,G2_tau)[withinRange]\n",
    "            \n",
    "        if returnAsTauFunction:\n",
    "            C_l.append(C_l_tau)\n",
    "        else:\n",
    "            C_l.append(integrate.trapz(C_l_tau,cosmoCont.taus_[withinRange],axis=0)) \n",
    "        \n",
    "    return np.array(C_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def C_l_Switched(lLimitForLimber,lVect, zLimits, kLimits, kRes, cosmoCont, corrType='TT', \n",
    "                 Pi=None, b=None, returnAsTauFunction=False):\n",
    "    \n",
    "    lVect_Bessel=[]\n",
    "    lVect_Limber=[]\n",
    "    \n",
    "    for l in lVect:\n",
    "        \n",
    "        if l>=lLimitForLimber:\n",
    "            lVect_Limber.append(l)\n",
    "        else:\n",
    "            lVect_Bessel.append(l)\n",
    "    \n",
    "    C_l=None\n",
    "    \n",
    "    if len(lVect_Bessel)>0:\n",
    "        \n",
    "        C_l=C_l_Bessel(lVect_Bessel, zLimits, kLimits, kRes, cosmoCont, corrType, Pi, b, returnAsTauFunction)\n",
    "    \n",
    "     \n",
    "    if len(lVect_Limber)>0:\n",
    "    \n",
    "        C_l_2=C_l_Limber(lVect_Limber, zLimits, cosmoCont, corrType, Pi, b, returnAsTauFunction)\n",
    "    \n",
    "        if C_l is None:\n",
    "            \n",
    "            C_l=C_l_2\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            C_l=np.concatenate((C_l,C_l_2))\n",
    "\n",
    "    return C_l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Signal to noise integrals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def SNIntegral(C_TT_CMB, fSky, lLimitForLimber, lVect, zLimits, kLimits, kRes, cosmoCont, Pi, b, returnBeforeSummation=False,\n",
    "               cosmoCont2=None):\n",
    "    #C_TT_CMB is expected to be provided for the same l-s that are in lVect\n",
    "    #If two models are provided, they are expected to have the same tau coverage\n",
    "    \n",
    "    C_GT=C_l_Switched(lLimitForLimber, lVect, zLimits, kLimits, kRes, cosmoCont, 'GT', Pi, b, returnAsTauFunction=True)\n",
    "    C_GG=C_l_Switched(lLimitForLimber, lVect, zLimits, kLimits, kRes, cosmoCont, 'GG', Pi, b, returnAsTauFunction=True)\n",
    "\n",
    "    if (np.any(C_GT[np.where(C_GG==0.0)]!=0.0)):\n",
    "    \n",
    "        raise ValueError('Error - nonzero C_GT, divided by zero C_GG')\n",
    "\n",
    "    else:\n",
    "        #All C_GTs are zero where C_GG are zero, setting C_GG to 1 to prevent division by zero\n",
    "        C_GG[np.where(C_GG==0.0)]=1.0\n",
    "    \n",
    "    if cosmoCont2 is not None:\n",
    "    \n",
    "        if not np.array_equal(cosmoCont2.zCurve_, cosmoCont.zCurve_):\n",
    "            \n",
    "            raise ValueError('Error, the redshift curves of the two provided cosmologies do not match')\n",
    "    \n",
    "        C_GT2=C_l_Switched(lLimitForLimber, lVect, zLimits, kLimits, kRes, cosmoCont2, 'GT', Pi, b, returnAsTauFunction=True)\n",
    "        C_GG2=C_l_Switched(lLimitForLimber, lVect, zLimits, kLimits, kRes, cosmoCont2, 'GG', Pi, b, returnAsTauFunction=True)\n",
    "\n",
    "        if (np.any(C_GT2[np.where(C_GG2==0.0)]!=0.0)):\n",
    "\n",
    "            raise ValueError('Error - nonzero C_GT2, divided by zero C_GG2')\n",
    "\n",
    "        else:\n",
    "            #All C_GTs are zero where C_GG are zero, setting C_GG to 1 to prevent division by zero\n",
    "            C_GG2[np.where(C_GG2==0.0)]=1.0\n",
    "\n",
    "        error1=(C_GG*C_TT_CMB.reshape(-1,1)/(2*lVect.reshape(-1,1)+1.0)/fSky)\n",
    "        error2=(C_GG2*C_TT_CMB.reshape(-1,1)/(2*lVect.reshape(-1,1)+1.0)/fSky)\n",
    "   \n",
    "        SNSqr_l_tau=(C_GT-C_GT2)**2/np.maximum(error1,error2)\n",
    "        \n",
    "    else:\n",
    "    \n",
    "        SNSqr_l_tau=C_GT*C_GT/(C_GG*C_TT_CMB.reshape(-1,1)/(2*lVect.reshape(-1,1)+1.0)/fSky)\n",
    "\n",
    "    withinRange=getIndicesInRedshiftRange(zLimits, cosmoCont)\n",
    "    \n",
    "    if returnBeforeSummation:\n",
    "        \n",
    "        return SNSqr_l_tau\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        return sqrt(np.sum(integrate.trapz(SNSqr_l_tau, cosmoCont.taus_[withinRange], axis=1), axis=0))\n",
    "    \n",
    "    \n",
    "def SNIntegralWithNoise(zLimitList, ObjCountList, C_TT_CMB, fSky, lLimitForLimber, lVect, kLimits, kRes, cosmoCont, Pi, b,\n",
    "                        returnBeforeSummation=False, cosmoCont2=None, Pi2=None, b2=None):\n",
    "    #C_TT_CMB is expected to be provided for the same l-s that are in lVect\n",
    "    #Pi_tau needs to be normalized to 1 integral (over tau) in each bin of zLimitList\n",
    "    \n",
    "\n",
    "    zLimitList=np.array(zLimitList)\n",
    "    zBoundaries=[np.amin(zLimitList),np.amax(zLimitList)]\n",
    "\n",
    "    C_GT=C_l_Switched(lLimitForLimber, lVect, zBoundaries, kLimits, kRes, cosmoCont, 'GT', Pi, b, returnAsTauFunction=True)\n",
    "    C_GG=C_l_Switched(lLimitForLimber, lVect, zBoundaries, kLimits, kRes, cosmoCont, 'GG', Pi, b, returnAsTauFunction=True)\n",
    "\n",
    "    if cosmoCont2 is not None:\n",
    "            \n",
    "        C_GT2=C_l_Switched(lLimitForLimber, lVect, zBoundaries, kLimits, kRes, cosmoCont2, 'GT', Pi2, b2, returnAsTauFunction=True)\n",
    "        C_GG2=C_l_Switched(lLimitForLimber, lVect, zBoundaries, kLimits, kRes, cosmoCont2, 'GG', Pi2, b2, returnAsTauFunction=True)\n",
    "    \n",
    "    SNSqr_l_bin=np.zeros([len(lVect),len(zLimitList)])\n",
    "    \n",
    "    for i in range(len(zLimitList)):\n",
    "    \n",
    "        if ObjCountList[i]<=0.0:\n",
    "            \n",
    "            SNSqr_l_bin[:,i]=0.0\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            withinRangeBoundary=getIndicesInRedshiftRange(zBoundaries, cosmoCont)\n",
    "\n",
    "            withinRangeCurrentBin=getIndicesInRedshiftRange(zLimitList[i], cosmoCont)\n",
    "\n",
    "            #Find matching indices\n",
    "            indexWithinBoundary=np.searchsorted(withinRangeBoundary, withinRangeCurrentBin)\n",
    "\n",
    "            C_GT_bin=integrate.trapz(C_GT[:,indexWithinBoundary], cosmoCont.taus_[withinRangeCurrentBin], axis=1)\n",
    "\n",
    "            C_GG_bin=(integrate.trapz(C_GG[:,indexWithinBoundary], cosmoCont.taus_[withinRangeCurrentBin], axis=1)\n",
    "                      +4*math.pi*fSky/ObjCountList[i])\n",
    "\n",
    "            if (np.any(C_GT_bin[np.where(C_GG_bin==0.0)]!=0.0)):\n",
    "\n",
    "                raise ValueError('Error - nonzero C_GT in bin '+str(i)+', divided by zero C_GG')\n",
    "\n",
    "            else:\n",
    "                #All C_GTs are zero where C_GG are zero, setting C_GG to 1 to prevent division by zero\n",
    "                C_GG_bin[np.where(C_GG_bin==0.0)]=1.0\n",
    "\n",
    "            if cosmoCont2 is not None:\n",
    "\n",
    "                withinRangeBoundary=getIndicesInRedshiftRange(zBoundaries, cosmoCont2)\n",
    "\n",
    "                withinRangeCurrentBin=getIndicesInRedshiftRange(zLimitList[i], cosmoCont2)\n",
    "\n",
    "                C_GT_bin2=integrate.trapz(C_GT2[:,indexWithinBoundary], cosmoCont2.taus_[withinRangeCurrentBin], axis=1)\n",
    "\n",
    "                C_GG_bin2=(integrate.trapz(C_GG2[:,indexWithinBoundary], cosmoCont2.taus_[withinRangeCurrentBin], axis=1)\n",
    "                          +4*math.pi*fSky/ObjCountList[i])\n",
    "\n",
    "                if (np.any(C_GT_bin2[np.where(C_GG_bin2==0.0)]!=0.0)):\n",
    "\n",
    "                    raise ValueError('Error - nonzero C_GT2 in bin '+str(i)+', divided by zero C_GG2')\n",
    "\n",
    "                else:\n",
    "                    #All C_GTs are zero where C_GG are zero, setting C_GG to 1 to prevent division by zero\n",
    "                    C_GG_bin2[np.where(C_GG_bin2==0.0)]=1.0\n",
    "\n",
    "                error1=(C_GG_bin*C_TT_CMB/(2*lVect+1.0)/fSky)\n",
    "                error2=(C_GG_bin2*C_TT_CMB/(2*lVect+1.0)/fSky)\n",
    "\n",
    "                SNSqr_l_bin[:,i]=(C_GT_bin-C_GT_bin2)**2/np.maximum(error1,error2)\n",
    "\n",
    "            else:\n",
    "\n",
    "                SNSqr_l_bin[:,i]=C_GT_bin*C_GT_bin/(C_GG_bin*C_TT_CMB/(2*lVect+1.0)/fSky)\n",
    "    \n",
    "    if returnBeforeSummation:\n",
    "        \n",
    "        return SNSqr_l_bin\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        return sqrt(np.sum(SNSqr_l_bin))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def SetupPiTau(piOption,zLimits,cosmoCont):\n",
    "    \n",
    "    zCurve=cosmoCont.zCurve_\n",
    "    tauCurve=cosmoCont.taus_\n",
    "        \n",
    "    withinRange=getIndicesInRedshiftRange(zLimits, cosmoCont)\n",
    "    \n",
    "    if piOption==1:\n",
    "        \n",
    "        ###Option 1\n",
    "        #Here we have a uniform dN/dz\n",
    "        dzCurve=zCurve[1:]-zCurve[:-1]\n",
    "\n",
    "        dzCentral=np.zeros(len(zCurve))\n",
    "\n",
    "        for j in range(len(zCurve)):\n",
    "\n",
    "            if j-1>=0 and j<len(dzCurve):\n",
    "\n",
    "                dzCentral[j]=(dzCurve[j-1]+dzCurve[j])/2.0\n",
    "\n",
    "        zRange=np.sum(dzCentral[withinRange])\n",
    "\n",
    "        Pi_z=np.zeros(len(tauCurve))\n",
    "\n",
    "        for j in withinRange:\n",
    "\n",
    "            Pi_z[j]=1.0/zRange\n",
    "\n",
    "        Pi_tau=Pi_z*np.gradient(zCurve,edge_order=2)/np.gradient(tauCurve,edge_order=2)\n",
    "\n",
    "    elif piOption==2:\n",
    "        \n",
    "        ###Option 2\n",
    "        #Here Pi_tau is used in place of r^2 n_C, but normalized to be integrated over tau\n",
    "        Pi_tau=np.zeros(len(tauCurve))\n",
    "\n",
    "        Pi_tau[withinRange]=1.0\n",
    "\n",
    "        Pi_tau/=integrate.trapz((tauCurve[-1]-tauCurve)**2*Pi_tau,tauCurve)\n",
    "\n",
    "        Pi_tau*=(tauCurve[-1]-tauCurve)**2\n",
    "\n",
    "    elif piOption==3:\n",
    "        \n",
    "        ###Option 3\n",
    "        #Here we use a pre-set dN/dz\n",
    "        Pi_tau=dN_dzNorm(zCurve,zCutIndex)*np.gradient(zCurve,edge_order=2)/np.gradient(tauCurve,edge_order=2)\n",
    "\n",
    "    elif piOption==4:\n",
    "        \n",
    "        ###Option 4\n",
    "        #Here we use a Gaussian dN/dz\n",
    "        mu=(zLimits[1]-zLimits[0])/2.0\n",
    "        sigma=mu/3.0\n",
    "\n",
    "        Pi_z=np.zeros(len(zCurve))\n",
    "        Pi_z[withinRange]=norm.pdf(zCurve[withinRange], loc=mu, scale=sigma)\n",
    "\n",
    "        Pi_z/=integrate.trapz(Pi_z[withinRange],zCurve[withinRange])\n",
    "\n",
    "        Pi_tau=Pi_z*np.gradient(zCurve,edge_order=2)/np.gradient(tauCurve,edge_order=2)\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        raise ValueError('Invalid piOption value provided. Valid options: 1,2,3,4')\n",
    "        \n",
    "    return Pi_tau\n",
    "\n",
    "def SetupPiTauForBinnedSN(piOption,zLimitList,cosmoCont):\n",
    "    #Setup a Pi_tau that is normalized to 1 integral (over tau) in each bin of zLimitList\n",
    "    \n",
    "    zCurve=cosmoCont.zCurve_\n",
    "    tauCurve=cosmoCont.taus_\n",
    "    \n",
    "    if piOption==1:\n",
    "        ###Option 1\n",
    "        #Here we have a uniform dN/dz\n",
    "    \n",
    "        Pi_z=np.zeros(len(tauCurve))\n",
    "    \n",
    "        for i in range(len(zLimitList)):\n",
    "\n",
    "            withinRangeCurrentBin=getIndicesInRedshiftRange(zLimitList[i], cosmoCont)\n",
    "\n",
    "            Pi_z[withinRangeCurrentBin[1:-1]]=1.0\n",
    "\n",
    "            Pi_z[withinRangeCurrentBin]/=integrate.trapz(Pi_z[withinRangeCurrentBin],zCurve[withinRangeCurrentBin])\n",
    "\n",
    "        Pi_tau=Pi_z*np.gradient(zCurve,edge_order=2)/np.gradient(tauCurve,edge_order=2)\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        raise ValueError('Invalid piOption value provided. Valid options: 1')\n",
    "        \n",
    "    return Pi_tau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def SetupGaussianObjCountForBinnedSN(mean,sigma,zLimitList,totalObjectNumber):\n",
    "    \n",
    "    binProbs=[]\n",
    "    \n",
    "    for i in range(len(zLimitList)):\n",
    "        \n",
    "        cumulProb=norm.cdf(zLimitList[i],loc=mean,scale=sigma)\n",
    "        \n",
    "        binProbs.append(cumulProb[1]-cumulProb[0])\n",
    "        \n",
    "    totalProb=np.sum(binProbs)\n",
    "    \n",
    "    ObjCountList=[]\n",
    "    \n",
    "    for i in range(len(zLimitList)):\n",
    "    \n",
    "        ObjCountList.append(totalObjectNumber*binProbs[i]/totalProb)\n",
    "    \n",
    "    return ObjCountList\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# S/N data for appendix B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lVect_Planck=np.arange(2,257)\n",
    "\n",
    "D_l_Planck_TT=np.loadtxt('data/Planck/COM_PowerSpect_CMB-base-plikHM-TT-lowTEB-minimum-theory_R2.02.txt')[0:len(lVect_Planck),1]/1e12\n",
    "\n",
    "C_l_Planck_TT=D_l_Planck_TT/(lVect_Planck*(lVect_Planck+1)/(2*math.pi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "containerID1='LCDM'\n",
    "containerID2='BR'\n",
    "redshiftRange=[0.0,9.0]\n",
    "\n",
    "#Pi_tau=SetupPiTau(1,redshiftRange,cosmoContainerDict[containerID])\n",
    "Pi_tau=np.ones(len(cosmoContainerDict[containerID1].taus_))\n",
    "b_tau=np.ones(len(Pi_tau))\n",
    "\n",
    "SN_Ideal_LCDM=SNIntegral(C_l_Planck_TT, 1.0,\n",
    "                         0, lVect_Planck, redshiftRange, \n",
    "                         cosmoContainerDict[containerID1].kLimits_, 1200, \n",
    "                         cosmoContainerDict[containerID1], Pi_tau, b_tau,\n",
    "                         returnBeforeSummation=False)\n",
    "\n",
    "SN_Ideal_AvERA=SNIntegral(C_l_Planck_TT, 1.0,\n",
    "                          0, lVect_Planck, redshiftRange, \n",
    "                          cosmoContainerDict[containerID2].kLimits_, 1200, \n",
    "                          cosmoContainerDict[containerID2], Pi_tau, b_tau,\n",
    "                          returnBeforeSummation=False)\n",
    "\n",
    "SN_Ideal_Diff=SNIntegral(C_l_Planck_TT, 1.0,\n",
    "                         0, lVect_Planck, redshiftRange, \n",
    "                         cosmoContainerDict[containerID1].kLimits_, 1200, \n",
    "                         cosmoContainerDict[containerID1], Pi_tau, b_tau,\n",
    "                         returnBeforeSummation=False, cosmoCont2=cosmoContainerDict[containerID2])\n",
    "\n",
    "print('z=0-9')\n",
    "print(SN_Ideal_LCDM,SN_Ideal_AvERA,SN_Ideal_Diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "containerID='LCDM'+str(0.00005)\n",
    "redshiftRange=[0.0,9.0]\n",
    "\n",
    "withinRange=getIndicesInRedshiftRange(redshiftRange, cosmoContainerDict[containerID])\n",
    "\n",
    "#Pi_tau=SetupPiTau(1,redshiftRange,cosmoContainerDict[containerID])\n",
    "Pi_tau=np.ones(len(cosmoContainerDict[containerID].taus_))\n",
    "b_tau=np.ones(len(Pi_tau))\n",
    "\n",
    "print(datetime.datetime.now())\n",
    "SN_l_tau_highRes2=SNIntegral(C_l_Planck_TT, 1.0, \n",
    "                             30, lVect_Planck, redshiftRange, \n",
    "                             cosmoContainerDict[containerID].kLimits_, 24000, \n",
    "                             cosmoContainerDict[containerID], Pi_tau, b_tau,\n",
    "                             returnBeforeSummation=True)\n",
    "print(datetime.datetime.now())\n",
    "\n",
    "sqrt(np.sum(integrate.trapz(SN_l_tau_highRes2, cosmoContainerDict[containerID].taus_[withinRange], axis=1), axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "containerID1='LCDM'\n",
    "containerID2='BR'\n",
    "redshiftRange=[0.0,1.5]\n",
    "\n",
    "#Pi_tau=SetupPiTau(1,redshiftRange,cosmoContainerDict[containerID])\n",
    "Pi_tau=np.ones(len(cosmoContainerDict[containerID1].taus_))\n",
    "b_tau=np.ones(len(Pi_tau))\n",
    "\n",
    "SN_Ideal_LCDM=SNIntegral(C_l_Planck_TT, 1.0,\n",
    "                         0, lVect_Planck, redshiftRange, \n",
    "                         cosmoContainerDict[containerID1].kLimits_, 1200, \n",
    "                         cosmoContainerDict[containerID1], Pi_tau, b_tau,\n",
    "                         returnBeforeSummation=False)\n",
    "\n",
    "SN_Ideal_AvERA=SNIntegral(C_l_Planck_TT, 1.0,\n",
    "                          0, lVect_Planck, redshiftRange, \n",
    "                          cosmoContainerDict[containerID2].kLimits_, 1200, \n",
    "                          cosmoContainerDict[containerID2], Pi_tau, b_tau,\n",
    "                          returnBeforeSummation=False)\n",
    "\n",
    "SN_Ideal_Diff=SNIntegral(C_l_Planck_TT, 1.0,\n",
    "                         0, lVect_Planck, redshiftRange, \n",
    "                         cosmoContainerDict[containerID1].kLimits_, 1200, \n",
    "                         cosmoContainerDict[containerID1], Pi_tau, b_tau,\n",
    "                         returnBeforeSummation=False, cosmoCont2=cosmoContainerDict[containerID2])\n",
    "\n",
    "print('z=0-1.5')\n",
    "print(SN_Ideal_LCDM,SN_Ideal_AvERA,SN_Ideal_Diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "containerID1='LCDM'\n",
    "containerID2='BR'\n",
    "redshiftRange=[0.0,1.0]\n",
    "\n",
    "fSky=1.0\n",
    "totalObjectNumber=1e7\n",
    "\n",
    "binWidth=[0.03,0.05,0.1,0.2]\n",
    "\n",
    "SNList_LCDM=[]\n",
    "SNList_AvERA=[]\n",
    "SNList_Diff=[]\n",
    "\n",
    "for binWidth in binWidths:\n",
    "\n",
    "    binEdges=np.arange(redshiftRange[0],redshiftRange[1]+1e-10,binWidth)\n",
    "\n",
    "    zLimitList=[]\n",
    "    ObjCountList=[]\n",
    "\n",
    "    for i in range(len(binEdges)-1):\n",
    "\n",
    "        zLimitList.append([binEdges[i],binEdges[i+1]])\n",
    "\n",
    "        ObjCountList.append(totalObjectNumber*(binEdges[i+1]-binEdges[i])/(binEdges[-1]-binEdges[0]))\n",
    "\n",
    "\n",
    "    Pi_tau=SetupPiTauForBinnedSN(1,zLimitList,cosmoContainerDict[containerID1])\n",
    "    b_tau=np.ones(len(Pi_tau))\n",
    "\n",
    "    SNSqr_l_bin=SNIntegralWithNoise(zLimitList, ObjCountList, \n",
    "                                    C_l_Planck_TT, fSky,\n",
    "                                    0, lVect_Planck,\n",
    "                                    cosmoContainerDict[containerID1].kLimits_, 1200, \n",
    "                                    cosmoContainerDict[containerID1], \n",
    "                                    Pi_tau, b_tau,\n",
    "                                    returnBeforeSummation=True)\n",
    "\n",
    "    SNList_LCDM.append(sqrt(np.sum(SNSqr_l_bin)))\n",
    "    \n",
    "    Pi_tau2=SetupPiTauForBinnedSN(1,zLimitList,cosmoContainerDict[containerID2])\n",
    "    b_tau2=np.ones(len(Pi_tau2))\n",
    "    \n",
    "    SNSqr_l_bin=SNIntegralWithNoise(zLimitList, ObjCountList, \n",
    "                                    C_l_Planck_TT, fSky,\n",
    "                                    0, lVect_Planck,\n",
    "                                    cosmoContainerDict[containerID2].kLimits_, 1200, \n",
    "                                    cosmoContainerDict[containerID2], \n",
    "                                    Pi_tau2, b_tau2,\n",
    "                                    returnBeforeSummation=True)\n",
    "    \n",
    "    SNList_AvERA.append(sqrt(np.sum(SNSqr_l_bin)))\n",
    "    \n",
    "    SNSqr_l_bin=SNIntegralWithNoise(zLimitList, ObjCountList, \n",
    "                                    C_l_Planck_TT, fSky,\n",
    "                                    0, lVect_Planck,\n",
    "                                    cosmoContainerDict[containerID1].kLimits_, 1200, \n",
    "                                    cosmoContainerDict[containerID1], \n",
    "                                    Pi_tau, b_tau,\n",
    "                                    returnBeforeSummation=True,\n",
    "                                    cosmoCont2=cosmoContainerDict[containerID2], Pi2=Pi_tau2, b2=b_tau2)\n",
    "    \n",
    "    SNList_Diff.append(sqrt(np.sum(SNSqr_l_bin)))\n",
    "    \n",
    "print(SNList_LCDM)\n",
    "print(SNList_AvERA)\n",
    "print(SNList_Diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "containerID1='LCDM'\n",
    "containerID2='BR'\n",
    "redshiftRange=[1.5,4.4]\n",
    "\n",
    "fSky=1.0\n",
    "\n",
    "totalObjectNumbers=np.logspace(5,8,100,base=10.0)\n",
    "binWidths=[0.1,2.9]\n",
    "\n",
    "\n",
    "SNList_LCDM=[]\n",
    "SNList_AvERA=[]\n",
    "SNList_Diff=[]\n",
    "\n",
    "for binWidth in binWidths:\n",
    "\n",
    "    SNList_LCDM.append([])\n",
    "    SNList_AvERA.append([])\n",
    "    SNList_Diff.append([])\n",
    "        \n",
    "    for totalObjectNumber in totalObjectNumbers:\n",
    "        \n",
    "        binEdges=np.arange(redshiftRange[0],redshiftRange[1]+1e-10,binWidth)\n",
    "\n",
    "        zLimitList=[]\n",
    "        ObjCountList=[]\n",
    "\n",
    "        for i in range(len(binEdges)-1):\n",
    "\n",
    "            zLimitList.append([binEdges[i],binEdges[i+1]])\n",
    "\n",
    "            ObjCountList.append(totalObjectNumber*(binEdges[i+1]-binEdges[i])/(binEdges[-1]-binEdges[0]))\n",
    "\n",
    "\n",
    "        Pi_tau=SetupPiTauForBinnedSN(1,zLimitList,cosmoContainerDict[containerID1])\n",
    "        b_tau=np.ones(len(Pi_tau))\n",
    "\n",
    "        SNSqr_l_bin=SNIntegralWithNoise(zLimitList, ObjCountList, \n",
    "                                        C_l_Planck_TT, fSky,\n",
    "                                        0, lVect_Planck,\n",
    "                                        cosmoContainerDict[containerID1].kLimits_, 1200, \n",
    "                                        cosmoContainerDict[containerID1], \n",
    "                                        Pi_tau, b_tau,\n",
    "                                        returnBeforeSummation=True)\n",
    "\n",
    "        SNList_LCDM[-1].append(sqrt(np.sum(SNSqr_l_bin)))\n",
    "\n",
    "        Pi_tau2=SetupPiTauForBinnedSN(1,zLimitList,cosmoContainerDict[containerID2])\n",
    "        b_tau2=np.ones(len(Pi_tau2))\n",
    "\n",
    "        SNSqr_l_bin=SNIntegralWithNoise(zLimitList, ObjCountList, \n",
    "                                        C_l_Planck_TT, fSky,\n",
    "                                        0, lVect_Planck,\n",
    "                                        cosmoContainerDict[containerID2].kLimits_, 1200, \n",
    "                                        cosmoContainerDict[containerID2], \n",
    "                                        Pi_tau2, b_tau2,\n",
    "                                        returnBeforeSummation=True)\n",
    "\n",
    "        SNList_AvERA[-1].append(sqrt(np.sum(SNSqr_l_bin)))\n",
    "\n",
    "        SNSqr_l_bin=SNIntegralWithNoise(zLimitList, ObjCountList, \n",
    "                                        C_l_Planck_TT, fSky,\n",
    "                                        0, lVect_Planck,\n",
    "                                        cosmoContainerDict[containerID1].kLimits_, 1200, \n",
    "                                        cosmoContainerDict[containerID1], \n",
    "                                        Pi_tau, b_tau,\n",
    "                                        returnBeforeSummation=True,\n",
    "                                        cosmoCont2=cosmoContainerDict[containerID2], Pi2=Pi_tau2, b2=b_tau2)\n",
    "\n",
    "        SNList_Diff[-1].append(sqrt(np.sum(SNSqr_l_bin)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(26.66,10))\n",
    "\n",
    "axs=[]\n",
    "\n",
    "axs.append(fig.add_subplot(121))\n",
    "axs.append(fig.add_subplot(122))\n",
    "\n",
    "for i in range(len(axs)):\n",
    "\n",
    "    ax=axs[i]\n",
    "    \n",
    "    ax.set_xlabel('Object number', fontsize=24)\n",
    "    ax.set_ylabel('S/N', fontsize=24)\n",
    "    ax.grid(True)\n",
    "    ax.set_xlim(1e5,1e8)\n",
    "    ax.set_ylim(0,10)\n",
    "\n",
    "    #ax.text(35,1.5e-10,'$z=0.48-0.58$', fontsize=20)\n",
    "\n",
    "    ax.semilogx(totalObjectNumbers, SNList_LCDM[i], '-', linewidth=3.0, color=simColors[1], label=plotLabels[1])\n",
    "    \n",
    "    ax.semilogx(totalObjectNumbers, SNList_AvERA[i], '-', linewidth=3.0, color=simColors[0], label=plotLabels[0])\n",
    "\n",
    "    ax.semilogx(totalObjectNumbers, SNList_Diff[i], '--', linewidth=3.0, color='black', label='Difference')\n",
    "    \n",
    "    ax.tick_params(axis='both', labelsize=20)\n",
    "    ax.tick_params(axis='x', pad=8)\n",
    "\n",
    "\n",
    "axs[0].set_title('Redshifts available', fontsize=28)\n",
    "axs[1].set_title('Redshifts unavailable', fontsize=28)\n",
    "\n",
    "axs[0].legend(shadow=False, fancybox=True, fontsize=24, loc=2)\n",
    "\n",
    "fig.savefig('signalToNoise.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Short redshift range integrals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "granettISWb_raw=np.loadtxt('ISWpower_Granett2008b.txt')\n",
    "\n",
    "valueNum=5\n",
    "\n",
    "granettISWbVals=granettISWb_raw[0:valueNum]\n",
    "\n",
    "granettISWbError=np.empty((valueNum*2,2))\n",
    "for i in range(valueNum):\n",
    "    granettISWbError[2*i]=granettISWb_raw[valueNum+i]\n",
    "    granettISWbError[2*i+1]=granettISWb_raw[2*valueNum+i]\n",
    "\n",
    "granettISWbError2=np.loadtxt('ISWpower_Granett2008bb.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "redshiftLimit=[0.48,0.58]\n",
    "sims=['BR','LCDM']\n",
    "plotLabels=['AvERA','$\\Lambda$CDM']\n",
    "\n",
    "lVect_short=np.arange(1,181)\n",
    "\n",
    "C_l_scaled_sim_short=[]\n",
    "C_l_scaled_sim_b_short=[]\n",
    "\n",
    "for i in range(len(sims)):\n",
    "\n",
    "    C_l_scaled_sim_short.append(C_l_Limber(lVect_short, redshiftLimit, \n",
    "                                           cosmoContainerDict[sims[i]], 'TT',\n",
    "                                           None, None, returnAsTauFunction=False))\n",
    "    \n",
    "    C_l_scaled_sim_b_short.append(C_l_Bessel(lVect_short, redshiftLimit, cosmoContainerDict[sims[i]].kLimits_, 1200, \n",
    "                                             cosmoContainerDict[sims[i]], 'TT',\n",
    "                                             None, None, returnAsTauFunction=False))\n",
    "    \n",
    "    C_l_scaled_sim_short[-1]=lVect_short*(lVect_short+1.0)/(2.0*math.pi)*C_l_scaled_sim_short[-1]\n",
    "    \n",
    "    C_l_scaled_sim_b_short[-1]=lVect_short*(lVect_short+1.0)/(2.0*math.pi)*C_l_scaled_sim_b_short[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(16,12))\n",
    "\n",
    "axs=[]\n",
    "axs.append(fig.add_subplot(111))\n",
    "\n",
    "for ax in [axs[0]]:\n",
    "\n",
    "    ax.set_xlabel('$l$', fontsize=24)\n",
    "    ax.set_ylabel('$l \\, (l+1) \\, C^{ISW}_l \\, (2\\pi)^{-1}$', fontsize=24)\n",
    "    ax.grid(True)\n",
    "    ax.set_xlim(1,180)\n",
    "    ax.set_ylim(5e-15,5e-10)\n",
    "\n",
    "    ax.text(35,1.5e-10,'$z=0.48-0.58$', fontsize=20)\n",
    "\n",
    "    ax.loglog(granettISWbVals[:,0],granettISWbVals[:,1]/1e12, 'ko', markersize=16)\n",
    "    for i in range(valueNum):\n",
    "        ax.loglog(granettISWbError[(2*i):(2*i+2),0],granettISWbError[(2*i):(2*i+2),1]/1e12, '-ko', markersize=10)\n",
    "        ax.loglog(granettISWbError2[(2*i):(2*i+2),0],granettISWbError2[(2*i):(2*i+2),1]/1e12, '-ko', markersize=10)\n",
    "    \n",
    "    for i in range(len(sims)):\n",
    "\n",
    "        ax.loglog(lVect_short,C_l_scaled_sim_b_short[i], '-', color=simColors[i], linewidth=3.0, label=plotLabels[i])\n",
    "        ax.loglog(lVect_short,C_l_scaled_sim_short[i], '--', color=simColors[i], linewidth=3.0)\n",
    "\n",
    "\n",
    "    ax.loglog(lVect_short,C_l_scaled_sim_short[i]*1e10, '--', color='black', linewidth=3.0, label=\"Limber\")\n",
    "    ax.loglog(lVect_short,C_l_scaled_sim_short[i]*1e10, '-', color='black', linewidth=3.0, label=\"Exact\")\n",
    "        \n",
    "    ax.tick_params(axis='both', labelsize=20)\n",
    "\n",
    "    ax.legend(shadow=False, fancybox=True, fontsize=24, loc=3)\n",
    "\n",
    "\n",
    "fig.savefig('newNotebook_Limber1.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(sims)):\n",
    "\n",
    "    C_l_Curve=interp1d(lVect, C_l_scaled_sim_b_short[i], kind='cubic', fill_value='extrapolate')\n",
    "\n",
    "    chiSqr=0.0\n",
    "    \n",
    "    for i in range(valueNum):\n",
    "    \n",
    "        chiSqr+=((C_l_Curve(granettISWbVals[i,0])-granettISWbVals[i,1]/1e12)/(granettISWbError[2*i+1,1]/1e12-granettISWbVals[i,1]/1e12))**2\n",
    "    \n",
    "    print(chiSqr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Literature comparison data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "AValues=[]\n",
    "lRanges=[]\n",
    "labels=[]\n",
    "edgecolors=[]\n",
    "edgestyles=[]\n",
    "facecolors=[]\n",
    "\n",
    "sdssColor='blue'\n",
    "wiseColor='xkcd:olive green'\n",
    "desColor='purple'\n",
    "mixedColor='xkcd:neon green'\n",
    "\n",
    "#brown pink olive cyan\n",
    "\n",
    "corrEdgeColor='black'\n",
    "stackingEdgeColor='black'\n",
    "\n",
    "corrEdgeStyle='dashed'\n",
    "stackingEdgeStyle='solid'\n",
    "\n",
    "alphaValue=0.1\n",
    "\n",
    "#deg=180/l\n",
    "#zero degrees plotted till 2500\n",
    "#zero l plotted till 1\n",
    "\n",
    "AValues.append([1.24,0.27])\n",
    "lRanges.append([180.0/12,180.0/0.25])\n",
    "labels.append('Giannantonio2008') #https://arxiv.org/pdf/0801.4380.pdf\n",
    "facecolors.append(mixedColor)\n",
    "\n",
    "AValues.append([1.38,0.32])\n",
    "lRanges.append([180.0/12,180.0/0.25])\n",
    "labels.append('Giannantonio2012') #https://arxiv.org/pdf/1209.2125.pdf\n",
    "facecolors.append(mixedColor)\n",
    "\n",
    "AValues.append([2.23,0.60])\n",
    "lRanges.append([6,180]) \n",
    "labels.append('Ho2008') #https://arxiv.org/pdf/0801.0642.pdf\n",
    "facecolors.append(mixedColor)\n",
    "\n",
    "AValues.append([2.51,1.25])\n",
    "lRanges.append([1,20])\n",
    "labels.append('Granett2009') #https://arxiv.org/pdf/0812.1025.pdf\n",
    "facecolors.append(sdssColor)\n",
    "\n",
    "AValues.append([1.74,1.46])\n",
    "lRanges.append([4,192])\n",
    "labels.append('Granett2009') #https://arxiv.org/pdf/0812.1025.pdf\n",
    "facecolors.append(sdssColor)\n",
    "\n",
    "AValues.append([0.8,0.9])\n",
    "lRanges.append([5,100]) \n",
    "labels.append('Kovacs2013') #https://arxiv.org/pdf/1301.0475.pdf\n",
    "facecolors.append(wiseColor)\n",
    "\n",
    "AValues.append([1.24,0.47])\n",
    "lRanges.append([10,100]) \n",
    "labels.append('Ferraro2015') #https://arxiv.org/pdf/1401.1193.pdf\n",
    "facecolors.append(wiseColor)\n",
    "\n",
    "AValues.append([0.88,0.74])\n",
    "lRanges.append([10,100]) \n",
    "labels.append('Ferraro2015') #https://arxiv.org/pdf/1401.1193.pdf\n",
    "facecolors.append(wiseColor)\n",
    "\n",
    "AValues.append([1.18,0.36])\n",
    "lRanges.append([2,100]) \n",
    "labels.append('Shajib2016') #https://arxiv.org/pdf/1604.03939.pdf\n",
    "facecolors.append(wiseColor)\n",
    "\n",
    "AValues.append([0.64,0.74])\n",
    "lRanges.append([2,100])\n",
    "labels.append('Shajib2016') #https://arxiv.org/pdf/1604.03939.pdf\n",
    "facecolors.append(wiseColor)\n",
    "\n",
    "AValues.append([3.4,1.1])\n",
    "lRanges.append([6,87])\n",
    "labels.append('Goto2012') #https://arxiv.org/pdf/1202.5306.pdf\n",
    "facecolors.append(wiseColor)\n",
    "\n",
    "AValues.append([2.4,1.0])\n",
    "lRanges.append([10,75])\n",
    "labels.append('Granett2015') #https://arxiv.org/pdf/1507.03914.pdf\n",
    "facecolors.append(sdssColor)\n",
    "\n",
    "AValues.append([1.0,0.25])\n",
    "lRanges.append([8,40])\n",
    "labels.append('Planck2015') #https://arxiv.org/pdf/1502.01595.pdf\n",
    "facecolors.append(mixedColor)\n",
    "\n",
    "for i in range(len(AValues)):\n",
    "    edgecolors.append(corrEdgeColor)\n",
    "    edgestyles.append(corrEdgeStyle)\n",
    "\n",
    "AValues.append([9.6/4.2,2.2/4.2]) #No error bar for LCDM prediction\n",
    "lRanges.append([22,49])\n",
    "labels.append('Granett2008') #https://arxiv.org/pdf/0805.3695.pdf\n",
    "facecolors.append(sdssColor)\n",
    "\n",
    "AValues.append([(2.9+9.3)/2.0,9.3-(2.9+9.3)/2.0])\n",
    "lRanges.append([12,128])\n",
    "labels.append('Papai2011') #https://arxiv.org/pdf/1012.3750.pdf\n",
    "facecolors.append(sdssColor)\n",
    "\n",
    "AValues.append([5.4,2.2]) # Not sure about this\n",
    "lRanges.append([10,128])\n",
    "labels.append('Granett2015') #https://arxiv.org/pdf/1507.03914.pdf\n",
    "facecolors.append(sdssColor)\n",
    "\n",
    "#14-29 at 150 Mpc/h\n",
    "#21-44 at 100 Mpc/h\n",
    "#35-74 at 60 Mpc/h\n",
    "#110-226 at 20 Mpc/h\n",
    "AValues.append([6.0,6.0/2.3])\n",
    "lRanges.append([14,226])\n",
    "labels.append('Cai2017') #https://arxiv.org/pdf/1609.00301.pdf\n",
    "facecolors.append(sdssColor)\n",
    "\n",
    "AValues.append([20.0,20.0/3.7])\n",
    "lRanges.append([14,74])\n",
    "labels.append('Cai2017') #https://arxiv.org/pdf/1609.00301.pdf\n",
    "facecolors.append(sdssColor)\n",
    "\n",
    "#9-19\n",
    "#97-197\n",
    "AValues.append([8.0,6.0])\n",
    "lRanges.append([10,197])\n",
    "labels.append('Kovacs2017') #https://arxiv.org/pdf/1610.00637.pdf\n",
    "facecolors.append(desColor)\n",
    "\n",
    "#15-32\n",
    "#126-308\n",
    "AValues.append([16.0,8.0])\n",
    "lRanges.append([15,308])\n",
    "labels.append('Kovacs2017') #https://arxiv.org/pdf/1610.00637.pdf\n",
    "facecolors.append(desColor)\n",
    "\n",
    "#AValues.append([1.64,0.53])\n",
    "#lRanges.append([???])\n",
    "#labels.append('Nadathur2016') #https://arxiv.org/pdf/1608.08638.pdf\n",
    "#facecolors.append(sdssColor)\n",
    "\n",
    "\n",
    "for i in range(len(AValues)-len(edgecolors)):\n",
    "    edgecolors.append(stackingEdgeColor)\n",
    "    edgestyles.append(stackingEdgeStyle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def computeHPDCI(samples,bins,interval=0.6827):\n",
    "    \n",
    "    (occurrences,binEdges)=np.histogram(samples,bins=bins)\n",
    "\n",
    "    hpdSum=0.0\n",
    "    totalSum=np.sum(occurrences)\n",
    "\n",
    "    minValue=np.inf\n",
    "    maxValue=-np.inf\n",
    "\n",
    "    ascendingIdx=np.argsort(occurrences)\n",
    "\n",
    "    for ind in reversed(ascendingIdx):\n",
    "\n",
    "        hpdSum+=occurrences[ind]\n",
    "\n",
    "        if minValue>binEdges[ind]:\n",
    "\n",
    "            minValue=binEdges[ind]\n",
    "\n",
    "        if maxValue<binEdges[ind+1]:\n",
    "\n",
    "            maxValue=binEdges[ind+1]\n",
    "\n",
    "        if hpdSum>totalSum*interval:\n",
    "            break\n",
    "\n",
    "    return np.array([minValue,maxValue,hpdSum/totalSum])\n",
    "\n",
    "\n",
    "def computeCIMedian(samples,bins,interval=0.6827):\n",
    "    \n",
    "    (occurrences,binEdges)=np.histogram(samples,bins=bins)\n",
    "\n",
    "    medValue=np.median(samples)\n",
    "    medInd=np.argmin(abs(binEdges-medValue))\n",
    "    \n",
    "    if binEdges[medInd]>medValue:\n",
    "        medInd-=1\n",
    "    \n",
    "    totalSum=np.sum(occurrences)\n",
    "    \n",
    "    ciSum=occurrences[medInd]\n",
    "    \n",
    "    minInd=medInd\n",
    "    maxInd=medInd\n",
    "\n",
    "    while ciSum<totalSum*interval:\n",
    "\n",
    "        if minInd-1>=0:\n",
    "            lowProb=occurrences[minInd-1]\n",
    "        else:\n",
    "            lowProb=0.0\n",
    "        \n",
    "        if maxInd+1<len(occurrences):\n",
    "            highProb=occurrences[maxInd+1]\n",
    "        else:\n",
    "            highProb=0.0\n",
    "        \n",
    "        if highProb>lowProb:\n",
    "            \n",
    "            ciSum+=highProb\n",
    "            maxInd+=1\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            ciSum+=lowProb\n",
    "            minInd-=1\n",
    "\n",
    "    return np.array([binEdges[minInd],binEdges[maxInd+1],ciSum/totalSum])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patches as patches\n",
    "\n",
    "sims=['BR','LCDM']\n",
    "plotLabels=['AvERA','$\\Lambda$CDM']\n",
    "\n",
    "randomSamples=1000000\n",
    "binNumber=500\n",
    "\n",
    "lowBound=5e-13\n",
    "\n",
    "fig = plt.figure(figsize=(26.66,10))\n",
    "\n",
    "axs=[]\n",
    "\n",
    "axs.append(fig.add_subplot(121))\n",
    "axs.append(fig.add_subplot(122))\n",
    "\n",
    "for ax in [axs[0]]:\n",
    "\n",
    "    ax.set_xlabel('$l$', fontsize=24)\n",
    "    ax.set_ylabel('$l \\, (l+1) \\, C^{ISW}_l \\, (2\\pi)^{-1}$', fontsize=24)\n",
    "    ax.grid(True)\n",
    "    ax.set_xlim(1,180)\n",
    "    ax.set_ylim(5e-15,5e-10)\n",
    "\n",
    "    ax.text(35,1.5e-10,'$z=0.48-0.58$', fontsize=20)\n",
    "\n",
    "    ax.loglog(granettISWbVals[:,0],granettISWbVals[:,1]/1e12, 'ko', markersize=16)\n",
    "    for i in range(valueNum):\n",
    "        ax.loglog(granettISWbError[(2*i):(2*i+2),0],granettISWbError[(2*i):(2*i+2),1]/1e12, '-ko', markersize=10)\n",
    "        ax.loglog(granettISWbError2[(2*i):(2*i+2),0],granettISWbError2[(2*i):(2*i+2),1]/1e12, '-ko', markersize=10)\n",
    "    \n",
    "    for i in range(len(sims)):\n",
    "\n",
    "        ax.loglog(lVect_short,C_l_scaled_sim_b_short[i], '-', color=simColors[i], linewidth=3.0, label=plotLabels[i])\n",
    "\n",
    "\n",
    "    ax.tick_params(axis='both', labelsize=20)\n",
    "\n",
    "    ax.legend(shadow=False, fancybox=True, fontsize=24, loc=3)\n",
    "\n",
    "\n",
    "for ax in [axs[1]]: \n",
    "\n",
    "    ax.add_patch(patches.Rectangle((-1e10,-1e10), #Bottom left coordinate\n",
    "                                   0.1, #Width\n",
    "                                   0.1, #Height\n",
    "                                   alpha=alphaValue,\n",
    "                                   facecolor=sdssColor,\n",
    "                                   fill=True,\n",
    "                                   label='SDSS'))\n",
    "\n",
    "    ax.add_patch(patches.Rectangle((-1e10,-1e10), #Bottom left coordinate\n",
    "                                   0.1, #Width\n",
    "                                   0.1, #Height\n",
    "                                   alpha=alphaValue,\n",
    "                                   facecolor=desColor,\n",
    "                                   fill=True,\n",
    "                                   label='DES'))\n",
    "\n",
    "    ax.add_patch(patches.Rectangle((-1e10,-1e10), #Bottom left coordinate\n",
    "                                   0.1, #Width\n",
    "                                   0.1, #Height\n",
    "                                   alpha=alphaValue,\n",
    "                                   facecolor=wiseColor,\n",
    "                                   fill=True,\n",
    "                                   label='WISE'))\n",
    "\n",
    "    ax.add_patch(patches.Rectangle((-1e10,-1e10), #Bottom left coordinate\n",
    "                                   0.1, #Width\n",
    "                                   0.1, #Height\n",
    "                                   alpha=alphaValue,\n",
    "                                   facecolor=mixedColor,\n",
    "                                   fill=True,\n",
    "                                   label='Mixed surveys'))\n",
    "    \n",
    "    if stackingEdgeColor is not None:\n",
    "        ax.add_patch(patches.Rectangle((-1e10,-1e10), #Bottom left coordinate\n",
    "                                       0.1, #Width\n",
    "                                       0.1, #Height\n",
    "                                       alpha=1.0,\n",
    "                                       edgecolor=stackingEdgeColor,\n",
    "                                       fill=False,\n",
    "                                       linewidth=1.0,\n",
    "                                       lineStyle=stackingEdgeStyle,\n",
    "                                       label='Stacking'))\n",
    "\n",
    "\n",
    "    if corrEdgeColor is not None:\n",
    "        ax.add_patch(patches.Rectangle((-1e10,-1e10), #Bottom left coordinate\n",
    "                                       0.1, #Width\n",
    "                                       0.1, #Height\n",
    "                                       alpha=1.0,\n",
    "                                       edgecolor=corrEdgeColor,\n",
    "                                       fill=False,\n",
    "                                       linewidth=1.0,\n",
    "                                       lineStyle=corrEdgeStyle,\n",
    "                                       label='Correlation'))\n",
    "    \n",
    "    for i in range(len(AValues)):\n",
    "        \n",
    "        ASamples=np.random.normal(loc=AValues[i][0],scale=AValues[i][1],size=randomSamples)\n",
    "        \n",
    "        #hpdci=computeHPDCI(ASamples**2,bins=binNumber)\n",
    "        cimedian=computeCIMedian(ASamples**2,bins=binNumber)        \n",
    "        \n",
    "        ell = []\n",
    "        clLow = []\n",
    "        clHigh = []\n",
    "        \n",
    "        for l in np.arange(lRanges[i][0],lRanges[i][1]+1,dtype=np.int32):\n",
    "            \n",
    "            ind=np.where(lVect==l)[0]\n",
    "            if len(ind)>0: #l=0,1 is missing from CMB power spectrum\n",
    "                \n",
    "                ell.append(lVect[ind[0]])\n",
    "                \n",
    "                if (C_l_scaled_sim_b[1][ind[0]]*cimedian[0])<lowBound:\n",
    "                    \n",
    "                    clLow.append(lowBound)\n",
    "                    \n",
    "                else:\n",
    "                    \n",
    "                    clLow.append( C_l_scaled_sim_b[1][ind[0]]*cimedian[0] )\n",
    "                \n",
    "                clHigh.append(C_l_scaled_sim_b[1][ind[0]]*cimedian[1] )\n",
    "\n",
    "        \n",
    "        xyPolygon=[]\n",
    "        \n",
    "        for j in range(len(ell)):\n",
    "\n",
    "            xyPolygon.append([ell[j], clLow[j]])\n",
    "\n",
    "        for j in reversed(range(len(ell))):\n",
    "\n",
    "            xyPolygon.append([ell[j], clHigh[j]])\n",
    "\n",
    "        j=0\n",
    "        xyPolygon.append([ell[j], clLow[j]])\n",
    "\n",
    "        xyPolygon=np.array(xyPolygon)\n",
    "\n",
    "\n",
    "        ax.add_patch(patches.Polygon(xyPolygon,\n",
    "                                     closed=True,\n",
    "                                     alpha=alphaValue,\n",
    "                                     facecolor=facecolors[i],\n",
    "                                     fill=True))\n",
    "\n",
    "        if (edgecolors[i] is not None):\n",
    "            \n",
    "            ax.add_patch(patches.Polygon(xyPolygon,\n",
    "                                         closed=True,\n",
    "                                         alpha=1.0,\n",
    "                                         edgecolor=edgecolors[i],\n",
    "                                         fill=False,\n",
    "                                         linewidth=1.0,\n",
    "                                         lineStyle=edgestyles[i]))\n",
    "\n",
    "    \n",
    "    for i in range(len(sims)):\n",
    "        ax.loglog(lVect,C_l_scaled_sim_b[i],linewidth=3.0, color=simColors[i])\n",
    "\n",
    "\n",
    "    ax.set_xlabel('$l$', fontsize=24)\n",
    "    ax.set_ylabel('$l \\, (l+1) \\, C^{ISW}_l \\, (2\\pi)^{-1}$', fontsize=24)\n",
    "    ax.grid(True)\n",
    "            \n",
    "    ax.tick_params(axis='both', labelsize=20)\n",
    "    ax.tick_params(axis='x', pad=8)   \n",
    "\n",
    "\n",
    "axs[1].set_xlim([1,100])\n",
    "axs[1].set_ylim([lowBound,2.5e-8])\n",
    "#axs[1].set_yticks(np.arange(0,4.1e-9,1e-9))\n",
    "                  \n",
    "axs[1].legend(shadow=False, fancybox=True, fontsize=24, loc=3)\n",
    "\n",
    "fig.savefig('newNotebook_ISWPowerLiterature_CI_Ver6.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full redshift range integrals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "redshiftLimit=[0.0,9.0]\n",
    "sims=['BR','LCDM']\n",
    "sims+=['Av320k','Av625k','Av1080k']\n",
    "\n",
    "lVect=np.arange(1,101)\n",
    "\n",
    "C_l_scaled_sim=[]\n",
    "C_l_scaled_sim_b=[]\n",
    "\n",
    "for i in range(len(sims)):\n",
    "\n",
    "    C_l_scaled_sim.append(C_l_Limber(lVect, redshiftLimit, \n",
    "                                           cosmoContainerDict[sims[i]], 'TT',\n",
    "                                           None, None, returnAsTauFunction=False))\n",
    "    \n",
    "    C_l_scaled_sim_b.append(C_l_Bessel(lVect, redshiftLimit, cosmoContainerDict[sims[i]].kLimits_, 1200, \n",
    "                                             cosmoContainerDict[sims[i]], 'TT',\n",
    "                                             None, None, returnAsTauFunction=False))\n",
    "    \n",
    "    C_l_scaled_sim[-1]=lVect*(lVect+1.0)/(2.0*math.pi)*C_l_scaled_sim[-1]\n",
    "    \n",
    "    C_l_scaled_sim_b[-1]=lVect*(lVect+1.0)/(2.0*math.pi)*C_l_scaled_sim_b[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patches as patches\n",
    "\n",
    "sims=['BR','LCDM']\n",
    "plotLabels=['AvERA','$\\Lambda$CDM']\n",
    "\n",
    "outRedshiftList=[0.5,1.0,1.5,2.0,2.5,3.0,4.0,5.0,6.0,7.0,8.0]\n",
    "\n",
    "startPointNum = 3 \n",
    "\n",
    "healPixResolution=64\n",
    "\n",
    "fig = plt.figure(figsize=(16,12))\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "\n",
    "tag='linear_'\n",
    "inPath= 'data/ISW_map_lin/'\n",
    "outPath= 'XXL_'\n",
    "outRedshiftListFull=outRedshiftList+[8.55]\n",
    "\n",
    "h_MXXL=0.73\n",
    "\n",
    "boxSize=3000/h_MXXL\n",
    "\n",
    "M_pointmass=8.456e9 #*0.73/h #In solar masses\n",
    "\n",
    "pointMassCount=303464448000.0\n",
    "    \n",
    "#Physical units in SI\n",
    "G_const=6.6740831e-11\n",
    "M_Sun=1.98892e30\n",
    "\n",
    "for zId in [len(outRedshiftListFull)-1]: #range(len(outRedshiftListFull)):     \n",
    "    \n",
    "    for i in range(len(sims)):\n",
    "        \n",
    "        H0_SI=cosmoContainerDict[sims[i]].H0_SI_\n",
    "\n",
    "        omega_m=cosmoContainerDict[sims[i]].omega_m_            \n",
    "\n",
    "         \n",
    "        massPrefactor1=3.0*(H0_SI**2)*omega_m\n",
    "        massPrefactor2=8.0*math.pi*G_const*pointMassCount*M_pointmass*M_Sun/(boxSize*MpcInMeters)**3\n",
    "        \n",
    "        cls=[]\n",
    "        ell=None\n",
    "         \n",
    "        for startID in range(startPointNum):                \n",
    "        \n",
    "            deltaT_matrix=np.loadtxt(inPath+outPath+'deltaT_'+tag+sims[i]+'_'+str(startID)+'.txt')\n",
    "            \n",
    "            #The factor 0.86037117794692208 scales the Millenium initial power spectrum to the LCDM one\n",
    "            #The massPrefactor terms scale the average density of the Universe to the theoretical cosmology calculation\n",
    "            #From CambTest.ipynb\n",
    "            cls.append(hp.anafast(deltaT_matrix[:,zId+2])*0.86037117794692208*(massPrefactor1/massPrefactor2)**2)\n",
    "            ell = np.arange(len(cls[-1]))\n",
    "\n",
    "            #ax.loglog(ell, ell * (ell+1) * cl / 2.0 / np.pi, '--')\n",
    "\n",
    "        cls=np.array(cls)\n",
    "            \n",
    "        xyPolygon=[]\n",
    "\n",
    "        for j in range(len(ell)):\n",
    "\n",
    "            xyPolygon.append([ell[j],ell[j] * (ell[j]+1) * np.amax(cls[:,j]) / 2.0 / np.pi])\n",
    "\n",
    "        for j in reversed(range(len(ell))):\n",
    "\n",
    "            xyPolygon.append([ell[j],ell[j] * (ell[j]+1) * np.amin(cls[:,j]) / 2.0 / np.pi])\n",
    "\n",
    "        j=0\n",
    "        xyPolygon.append([ell[j],ell[j] * (ell[j]+1) * np.amax(cls[:,j]) / 2.0 / np.pi])\n",
    "\n",
    "        xyPolygon=np.array(xyPolygon)\n",
    "\n",
    "\n",
    "        ax.add_patch(patches.Polygon(xyPolygon,\n",
    "                                     closed=True,\n",
    "                                     alpha=0.3,\n",
    "                                     facecolor=simColors[i],\n",
    "                                     fill=True))\n",
    "            \n",
    "    ax.set_xlabel('$l$', fontsize=24)\n",
    "    ax.set_ylabel('$l \\, (l+1) \\, C^{ISW}_l \\, (2\\pi)^{-1}$', fontsize=24)\n",
    "    ax.grid(True)\n",
    "    ax.set_xlim(1,100)\n",
    "    ax.set_ylim(5e-13,5e-9)\n",
    "\n",
    "    for i in range(len(sims)):\n",
    "\n",
    "        ax.loglog(lVect,C_l_scaled_sim_b[i], '-', color=simColors[i], linewidth=3.0, label=plotLabels[i])\n",
    "\n",
    "\n",
    "    ax.tick_params(axis='both', labelsize=20)\n",
    "\n",
    "    ax.legend(shadow=False, fancybox=True, fontsize=24, loc=1)\n",
    "    \n",
    "    fig.savefig('newNotebook_ISWPowerFinal.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patches as patches\n",
    "\n",
    "sims=['BR','LCDM']\n",
    "plotLabels=['AvERA','$\\Lambda$CDM']\n",
    "\n",
    "outRedshiftList=[0.5,1.0,1.5,2.0,2.5,3.0,4.0,5.0,6.0,7.0,8.0]\n",
    "\n",
    "startPointNum = 3 \n",
    "\n",
    "healPixResolution=64\n",
    "\n",
    "fig = plt.figure(figsize=(16,12))\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "\n",
    "tag='linear_'\n",
    "inPath= 'data/ISW_map_lin/'\n",
    "outPath= 'XXL_'\n",
    "outRedshiftListFull=outRedshiftList+[8.55]\n",
    "\n",
    "h_MXXL=0.73\n",
    "\n",
    "boxSize=3000/h_MXXL\n",
    "\n",
    "M_pointmass=8.456e9 #*0.73/h #In solar masses\n",
    "\n",
    "pointMassCount=303464448000.0\n",
    "    \n",
    "#Physical units in SI\n",
    "G_const=6.6740831e-11\n",
    "M_Sun=1.98892e30\n",
    "\n",
    "for zId in [len(outRedshiftListFull)-1]: #range(len(outRedshiftListFull)):     \n",
    "    \n",
    "    for i in range(len(sims)):\n",
    "        \n",
    "        H0_SI=cosmoContainerDict[sims[i]].H0_SI_\n",
    "\n",
    "        omega_m=cosmoContainerDict[sims[i]].omega_m_            \n",
    "\n",
    "         \n",
    "        massPrefactor1=3.0*(H0_SI**2)*omega_m\n",
    "        massPrefactor2=8.0*math.pi*G_const*pointMassCount*M_pointmass*M_Sun/(boxSize*MpcInMeters)**3\n",
    "        \n",
    "        cls=[]\n",
    "        ell=None\n",
    "         \n",
    "        for startID in range(startPointNum):                \n",
    "        \n",
    "            deltaT_matrix=np.loadtxt(inPath+outPath+'deltaT_'+tag+sims[i]+'_'+str(startID)+'.txt')\n",
    "\n",
    "            #The factor 0.86037117794692208 scales the Millenium initial power spectrum to the LCDM one\n",
    "            #The massPrefactor terms scale the average density of the Universe to the theoretical cosmology calculation\n",
    "            #From CambTest.ipynb\n",
    "            cls.append(hp.anafast(deltaT_matrix[:,zId+2])*0.86037117794692208*(massPrefactor1/massPrefactor2)**2)\n",
    "            ell = np.arange(len(cls[-1]))\n",
    "\n",
    "            #ax.loglog(ell, ell * (ell+1) * cl / 2.0 / np.pi, '--')\n",
    "\n",
    "        cls=np.array(cls)\n",
    "            \n",
    "        xyPolygon=[]\n",
    "\n",
    "        for j in range(len(ell)):\n",
    "\n",
    "            xyPolygon.append([ell[j],ell[j] * (ell[j]+1) * np.amax(cls[:,j]) / 2.0 / np.pi])\n",
    "\n",
    "        for j in reversed(range(len(ell))):\n",
    "\n",
    "            xyPolygon.append([ell[j],ell[j] * (ell[j]+1) * np.amin(cls[:,j]) / 2.0 / np.pi])\n",
    "\n",
    "        j=0\n",
    "        xyPolygon.append([ell[j],ell[j] * (ell[j]+1) * np.amax(cls[:,j]) / 2.0 / np.pi])\n",
    "\n",
    "        xyPolygon=np.array(xyPolygon)\n",
    "\n",
    "\n",
    "        ax.add_patch(patches.Polygon(xyPolygon,\n",
    "                                     closed=True,\n",
    "                                     alpha=0.3,\n",
    "                                     facecolor=simColors[i],\n",
    "                                     fill=True))\n",
    "            \n",
    "    ax.set_xlabel('$l$', fontsize=24)\n",
    "    ax.set_ylabel('$l \\, (l+1) \\, C^{ISW}_l \\, (2\\pi)^{-1}$', fontsize=24)\n",
    "    ax.grid(True)\n",
    "    ax.set_xlim(1,100)\n",
    "    ax.set_ylim(5e-13,5e-9)\n",
    "\n",
    "    for i in range(len(sims)):\n",
    "\n",
    "        ax.loglog(lVect,C_l_scaled_sim_b[i], '-', color=simColors[i], linewidth=3.0, label=plotLabels[i])\n",
    "        ax.loglog(lVect,C_l_scaled_sim[i], '--', color=simColors[i], linewidth=3.0)\n",
    "\n",
    "    ax.loglog(lVect,C_l_scaled_sim[i]*1e10, '--', color='black', linewidth=3.0, label=\"Limber\")\n",
    "    ax.loglog(lVect,C_l_scaled_sim[i]*1e10, '-', color='black', linewidth=3.0, label=\"Exact\")\n",
    "        \n",
    "    ax.tick_params(axis='both', labelsize=20)\n",
    "\n",
    "    ax.legend(shadow=False, fancybox=True, fontsize=24, loc=1)\n",
    "    \n",
    "    fig.savefig('newNotebook_Limber2.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sims=['BR','LCDM']\n",
    "plotLabels=['AvERA','$\\Lambda$CDM']\n",
    "simColors=['blue','red']\n",
    "\n",
    "sims+=['Av320k','Av625k','Av1080k']\n",
    "simColors+=['Aqua','Blue','Teal']\n",
    "\n",
    "plotLabels+=['$3.96 \\\\times 10^{11} M_\\odot, \\, H_0-2\\sigma$',\n",
    "             '$2.03 \\\\times 10^{11} M_\\odot, \\, H_0-1\\sigma$',\n",
    "             '$1.17 \\\\times 10^{11} M_\\odot, \\, H_0$']\n",
    "\n",
    "fig = plt.figure(figsize=(16,12))\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "            \n",
    "ax.set_xlabel('$l$', fontsize=24)\n",
    "ax.set_ylabel('$l \\, (l+1) \\, C^{ISW}_l \\, (2\\pi)^{-1}$', fontsize=24)\n",
    "ax.grid(True)\n",
    "ax.set_xlim(1,100)\n",
    "ax.set_ylim(5e-13,5e-9)\n",
    "\n",
    "for i in reversed(range(1,len(sims))):\n",
    "\n",
    "    ax.loglog(lVect,C_l_scaled_sim_b[i], '-', color=simColors[i], linewidth=3.0, label=plotLabels[i])\n",
    "\n",
    "\n",
    "ax.tick_params(axis='both', labelsize=20)\n",
    "\n",
    "ax.legend(shadow=False, fancybox=True, fontsize=24, loc=3)\n",
    "\n",
    "fig.savefig('newNotebook_ParticleMass.pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ISW map plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sims=['BR','LCDM']\n",
    "simLabels=['AvERA','LCDM']\n",
    "plotLabels=['AvERA','$\\Lambda$CDM']\n",
    "\n",
    "\n",
    "outRedshiftList=[0.5,1.0,1.5,2.0,2.5,3.0,4.0,5.0,6.0,7.0,8.0]\n",
    "\n",
    "startPointNum = 3 \n",
    "\n",
    "simulationTypes = 3\n",
    "\n",
    "healPixResolution=64\n",
    "\n",
    "scaleMultiplier=1e6\n",
    "\n",
    "isLinear=True\n",
    "simulationType=2\n",
    "\n",
    "if isLinear:\n",
    "    tag='linear_'\n",
    "else:\n",
    "    tag=''   \n",
    "\n",
    "\n",
    "if simulationType==0:\n",
    "\n",
    "    inPath= 'data/LC_results/'\n",
    "    outPath= 'data/ISW_map/'\n",
    "\n",
    "    outRedshiftListFull=outRedshiftList+[8.71]\n",
    "\n",
    "elif simulationType==1:\n",
    "\n",
    "    inPath= 'data/LC_results_XXL/'\n",
    "    outPath= 'data/ISW_map/XXL_'\n",
    "\n",
    "    outRedshiftListFull=outRedshiftList+[8.55]\n",
    "    \n",
    "elif simulationType==2:\n",
    "\n",
    "    inPath= 'data/LC_results_XXL_lin/'\n",
    "    outPath= 'data/ISW_map_lin/XXL_'\n",
    "\n",
    "    outRedshiftListFull=outRedshiftList+[8.55]\n",
    "\n",
    "zId=len(outRedshiftListFull)-1\n",
    "   \n",
    "\n",
    "fig = plt.figure(figsize=(len(sims)*5,startPointNum*2.6))\n",
    "\n",
    "#fig.suptitle('ISW integrated to z = '+str(outRedshiftListFull[zId]), fontsize=\"20\")\n",
    "\n",
    "(rangeMin,rangeMax)=(1e10,-1e10)\n",
    "\n",
    "for startID in range(startPointNum):\n",
    "\n",
    "    for i in range(len(sims)):\n",
    "\n",
    "        H0_SI=cosmoContainerDict[sims[i]].H0_SI_\n",
    "\n",
    "        omega_m=cosmoContainerDict[sims[i]].omega_m_\n",
    "         \n",
    "        massPrefactor1=3.0*(H0_SI**2)*omega_m\n",
    "        massPrefactor2=8.0*math.pi*G_const*pointMassCount*M_pointmass*M_Sun/(boxSize*MpcInMeters)**3\n",
    "        \n",
    "        deltaT_matrix=np.loadtxt(outPath+'deltaT_'+tag+sims[i]+'_'+str(startID)+'.txt')*(massPrefactor1/massPrefactor2)\n",
    "\n",
    "        (localRangeMin,localRangeMax)=numpy.percentile(deltaT_matrix[:,zId+2],[3,97])\n",
    "\n",
    "        rangeMin=min(rangeMin,localRangeMin)\n",
    "        rangeMax=max(rangeMax,localRangeMax)\n",
    "\n",
    "for startID in range(startPointNum):\n",
    "\n",
    "    for i in range(len(sims)):\n",
    "\n",
    "        H0_SI=cosmoContainerDict[sims[i]].H0_SI_\n",
    "\n",
    "        omega_m=cosmoContainerDict[sims[i]].omega_m_\n",
    "         \n",
    "        massPrefactor1=3.0*(H0_SI**2)*omega_m\n",
    "        massPrefactor2=8.0*math.pi*G_const*pointMassCount*M_pointmass*M_Sun/(boxSize*MpcInMeters)**3\n",
    "        \n",
    "        #ax = fig.add_subplot(startPointNum,len(sims),startID*len(sims)+i+1)\n",
    "\n",
    "        deltaT_matrix=np.loadtxt(outPath+'deltaT_'+tag+sims[i]+'_'+str(startID)+'.txt')*(massPrefactor1/massPrefactor2)\n",
    "        \n",
    "        unitStr='$\\Delta T_{\\mathrm{ISW}} \\, [\\mu K]$'       \n",
    "        \n",
    "        #plt.axes(ax)\n",
    "        hp.mollview(deltaT_matrix[:,zId+2]*scaleMultiplier,\n",
    "                    title='',\n",
    "                    unit='',\n",
    "                    cbar=False,\n",
    "                    #hold=True,\n",
    "                    min=rangeMin*scaleMultiplier,\n",
    "                    max=rangeMax*scaleMultiplier,\n",
    "                    sub=(startPointNum,len(sims),startID*len(sims)+i+1))\n",
    "\n",
    "        if startID==2 and i==0:\n",
    "            \n",
    "            im = fig.axes[-1].get_images()[0]\n",
    "        \n",
    "            colorAx = fig.add_axes([0.25, -0.1, 0.5, 0.05])\n",
    "\n",
    "            cb=fig.colorbar(im, cax=colorAx, orientation='horizontal')\n",
    "            \n",
    "            cb.ax.text(0.5,-1.5,unitStr,fontsize=22,transform=cb.ax.transAxes,ha='center',va='center')\n",
    "            cb.ax.tick_params(labelsize=18)\n",
    "    \n",
    "fig.text(0.1,-0.05,plotLabels[0], ha='center', fontsize=22)\n",
    "fig.text(0.9,-0.05,plotLabels[1], ha='center', fontsize=22)\n",
    "\n",
    "fig.savefig('newNotebook_ISWmap.pdf', bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
